---
title: Causal Computational Asymmetry - train two networks, the faster one is the cause
date: 2026-02-25 14:54:00 +0000
image:
  path: assets/img/posts/causal-landscape.png
  class: "img-right"
categories: [AI]
tags: [AI]  
---

okay so here's a question that sounds trivially stupid until you actually sit with it. two variables are correlated. which one causes the other? your first instinct is "just run an experiment." fair. but what if you can't? what if all you have is a dataset of things that already happened, no randomization, no control group, just observations? can you figure out the causal arrow from the data alone?

this shows up everywhere. does poverty cause poor health, or do health problems trap people in poverty? does social media use cause depression, or do depressed people spend more time on social media? does sleep deprivation impair cognition, or does cognitive stress disrupt sleep? does inflammation drive heart disease, or does cardiovascular damage trigger inflammation? you can collect data for a hundred years and not answer these from observation alone. and the reason for that is a mathematical theorem proved in 1995 by a computer scientist named Judea Pearl.

what this paper does is use a completely new signal to crack the bivariate causal direction problem. the signal is how fast a neural network converges during training. training a network to predict $Y$ from $X$ takes fewer gradient descent steps than training a network to predict $X$ from $Y$, when $X$ is the actual cause of $Y$. this isn't just an observation. it's a proved theorem with three chained lemmas. the paper is "Causal Direction from Convergence Time" by Abdulrahman Tamim, February 2026, and it's one of the more interesting things that's happened in causal discovery in years.


## The ladder of causation and why GPT-4 is permanently stuck at the bottom

Judea Pearl won the Turing Award in 2011. his main contribution is basically inventing the rigorous mathematical language for reasoning about cause and effect. his 2000 textbook *Causality* and the 2018 popular book *The Book of Why* both frame causal thinking around what he calls the Ladder of Causation. the Causal Hierarchy Theorem, proved formally by Bareinboim, Correa, Ibeling, and Icard in 2022, shows that the rungs of this ladder almost always separate: lower-rung information almost never answers higher-rung questions. "almost always" means for all but a measure-zero set of distributions. in practice, always.

**rung one is seeing.** mathematically this is $P(Y \mid X = x)$. you observe data and ask: given that $X$ happened to take value $x$, what does the distribution of $Y$ look like? this is every statistical method ever. regression, classification, neural networks, language models, image generators, recommendation systems, all of it. GPT-4, every transformer, every diffusion model. everything in modern ML lives here. you're modeling conditional distributions learned from historical data. you can be extraordinarily good at this and still be completely blind to causation.

**rung two is doing.** mathematically this is $P(Y \mid \mathrm{do}(X = x))$. the $\mathrm{do}(\cdot)$ operator looks superficially like conditioning but it's doing something categorically different. when you condition on $X = x$, you're just filtering your dataset to rows where $X$ happened to take that value. all the background factors that influence $X$ are still operating. people who smoke more might be in more stressful jobs. people who exercise more might also eat better. you're not controlling anything, you're looking at a slice of existing correlations. when you $\mathrm{do}(X = x)$, you surgically cut every arrow pointing into $X$, force its value to $x$, and ask what $Y$ does. a randomized controlled trial physically implements this. random assignment to treatment or placebo breaks every association between who-gets-treated and every other variable in existence, because treatment is now determined by a coin flip rather than by anything about the person. that's why RCTs are the gold standard and why observational studies always fight confounding.

the do-calculus has three rules. rule 1 (insertion/deletion of observations) says $P(y \mid \mathrm{do}(x), z, w) = P(y \mid \mathrm{do}(x), w)$ if $Y$ is independent of $Z$ given $X, W$ in the graph with all arrows into $X$ deleted. rule 2 (action/observation exchange) says $P(y \mid \mathrm{do}(x), \mathrm{do}(z), w) = P(y \mid \mathrm{do}(x), z, w)$ under a d-separation condition. rule 3 (insertion/deletion of actions) allows removing do-operators under another d-separation condition. Shpitser and Pearl proved in 2006 that these three rules are complete: if you can't reduce $P(y \mid \mathrm{do}(x))$ to a purely observational expression using these rules, then the causal effect is not identifiable from observational data, period. no other method can do it non-parametrically. this isn't a limitation of the calculus. it's a characterization of what's fundamentally possible.

**rung three is imagining.** this is $P(Y_{X=x'} \mid X = x, Y = y)$. counterfactuals. you took the drug and recovered. would you have recovered without it? you took this job. would you have been happier at the other one? the subscript $Y_{X=x'}$ means the value $Y$ would have taken in a world where $X$ was forced to $x'$, while conditioning on the fact that in the actual observed world $X = x$ and $Y = y$. these two worlds never coexisted. you can't run the experiment. this rung is the basis of moral responsibility, legal causation, and regret. medical malpractice cases live here. policy evaluation under distribution shift lives here. counterfactual RL lives here.

the core impossibility: Pearl proved in 1995, formalized via the do-calculus completeness, that no algebraic manipulation you can apply to any observational conditional distribution $P(Y \mid X)$ can ever produce $P(Y \mid \mathrm{do}(X))$ without structural assumptions about the causal graph. you could have a trillion data points and still not answer "what happens if I force $X$ to change" without knowing the causal structure. this is the "no causes in, no causes out" principle in the causal discovery literature. GPT-4, with its entire training corpus of human knowledge, lives on rung 1. it can tell you that smokers tend to get lung cancer. it cannot, by mathematical impossibility, tell you what happens to lung cancer rates if you force everyone to stop smoking.

<img width="1232" height="970" alt="image" src="https://github.com/user-attachments/assets/a6bcdb45-e932-4b3a-8bbd-77aa2d60c014" />


CCA doesn't solve causal discovery in general. it solves the first question you need to answer before any rung-2 reasoning is possible: given two variables, which direction does the arrow point? before the do-calculus can compute $P(Y \mid \mathrm{do}(X))$, you need to know whether the edge is $X \to Y$ or $Y \to X$. that's the problem.



## Prerequisites: everything you need to understand the math in this blog

this section is written assuming you have seen algebra and basic calculus but not much else. every concept in the main blog that might trip you up is here, explained from zero. you can read this first, or come back to it when something in the main text doesn't click. either way it's all here.



### What a neural network is

a neural network is a mathematical function made of layers. each layer takes a vector of numbers, multiplies by a weight matrix, adds a bias vector, and then applies a nonlinear function elementwise. repeat this $L$ times:

$$a^{(1)} = \sigma(W^{(1)} x + b^{(1)})$$
$$a^{(2)} = \sigma(W^{(2)} a^{(1)} + b^{(2)})$$
$$\vdots$$
$$\hat{y} = W^{(L)} a^{(L-1)} + b^{(L)}$$

the matrices $W^{(\ell)}$ and vectors $b^{(\ell)}$ are the parameters (weights) you learn. $\sigma$ is the activation function — a nonlinearity applied elementwise. without $\sigma$, the whole thing collapses to a single linear function (stacking linear maps gives a linear map). the activation makes the network capable of representing nonlinear functions.

tanh is one common activation: $\sigma(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$. it outputs values between $-1$ and $1$, is smooth (differentiable everywhere), and saturates for large $|z|$. ReLU is another: $\sigma(z) = \max(0, z)$. outputs zero for negative inputs, identity for positive. computationally cheap, doesn't saturate for positive inputs, widely used in practice.

an MLP-64-64-Tanh means: multilayer perceptron with two hidden layers, each with 64 neurons, tanh activations. the 64s refer to the number of neurons (the dimension of the vectors $a^{(\ell)}$) in each hidden layer.



### Loss functions, MSE, and what convergence means

a loss function measures how bad the network's current predictions are. the mean squared error (MSE) for a regression problem is:

$$\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^n \left(\hat{y}_i - y_i\right)^2$$

where $\hat{y}_i = g_\theta(x_i)$ is the network's prediction and $y_i$ is the true value. you square the errors so that large errors are penalized more than small ones, and so that positive and negative errors don't cancel.

the population version (what you're really trying to minimize, the loss over the true data distribution not just your training sample) is:

$$\mathcal{L}(\theta) = E\!\left[(g_\theta(X) - Y)^2\right]$$

the best possible function to minimize this over all functions (not just networks) is $g^*(X) = E[Y \mid X]$, the conditional expectation of $Y$ given $X$. the proof: $(g(X) - Y)^2 = (g(X) - E[Y \mid X] + E[Y \mid X] - Y)^2$, expand, take expectation, the cross term vanishes by the tower property, and you're left with $E[(g(X) - E[Y \mid X])^2] + E[(E[Y \mid X] - Y)^2]$. the second term is fixed (it's the irreducible noise $\text{Var}(Y \mid X)$). the first term is minimized to zero by setting $g = E[Y \mid X]$.

convergence means the training process has driven the loss close enough to its minimum that it's not meaningfully decreasing anymore. in the CCA framework, convergence is defined operationally: the held-out validation loss drops below a threshold $\tau$. "held-out" means you evaluate on data the network hasn't trained on — this gives an honest estimate of how well the network is actually learning the function, not just memorizing training examples.

the minimum achievable held-out MSE is the irreducible noise variance $\sigma_\varepsilon^2$ — the variance of the noise $\varepsilon$ in $Y = f(X) + \varepsilon$. you can't predict $\varepsilon$ because it's independent of $X$. the best you can do is learn $f$ and accept $\varepsilon$ as irreducible error.



### gradient descent: the update rule

gradient descent is the algorithm that trains neural networks. start with random parameters $\theta_0$. at each step $t$, compute the gradient of the loss with respect to the current parameters, and update:

$$\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)$$

$\eta$ is the learning rate: how big a step you take. too large and you overshoot the minimum and oscillate. too small and it takes forever to converge.

mini-batch gradient descent uses a random subset (mini-batch) of size $m$ from the training data to estimate the gradient at each step:

$$\hat{\nabla}_\theta \mathcal{L} \approx \frac{1}{m} \sum_{i \in \text{batch}} \nabla_\theta \left(g_\theta(x_i) - y_i\right)^2$$

using the full dataset for each step (batch gradient descent) is exact but expensive for large datasets. using one sample (stochastic gradient descent, SGD) is cheap but very noisy. mini-batch is the practical compromise.

Adam (Adaptive Moment Estimation) is a smarter optimizer than plain SGD. it keeps a running estimate of the mean and variance of each gradient component and uses them to adapt the effective learning rate per parameter. parameters with consistently large gradients get smaller effective learning rates (they're already moving fast). parameters with small or inconsistent gradients get larger effective learning rates (they need more help). this makes Adam much more robust to the choice of $\eta$ compared to plain SGD.

RMSProp is similar to Adam: it normalizes the learning rate by the running estimate of the squared gradient. the practical effect is similar — adaptive per-parameter learning rates that prevent the optimization from being dominated by a few large-gradient parameters.

<img width="1235" height="479" alt="image" src="https://github.com/user-attachments/assets/2c4333c2-5c3b-4920-a8d1-7efedb734d6b" />


### what a loss curve is and how to read it

a loss curve is just a plot of the loss value on the y-axis and the training step (iteration number) on the x-axis. it shows how the network's error changes as it trains.

a healthy loss curve starts high (the network is making random predictions at initialization) and decreases, typically fast at first and then slowing as it approaches the minimum. this is the geometric decay described by the PL condition in the blog: loss decreases as $(1 - 2\eta\mu)^t$ times the initial gap, which is fast early and slow near the minimum.

a plateau in the loss curve means the network has stopped making progress. this could mean it converged (reached the minimum), or it could mean it got stuck in a saddle point or flat region. the CCA method uses this distinction: the forward direction converges (loss drops below threshold $\tau$), the reverse direction often plateaus above $\tau$ and never converges within the step cap.

why log scale on the y-axis? when loss spans many orders of magnitude (say, from 1 down to 0.001), a linear y-axis makes the early fast descent look like a cliff and the later slow descent invisible. log scale makes the geometric decay linear-looking and shows the full dynamic range.

<img width="1200" height="670" alt="fig_loss_curves" src="https://github.com/user-attachments/assets/c3d9e978-8bb3-4cc6-bb7b-f842a75014ba" />


### What the Polyak-Łojasiewicz condition actually says

the PL condition is a way of saying "the loss landscape is well-behaved near the minimum." specifically it says the gradient must be large if you're far from the minimum:

$$\|\nabla \mathcal{L}(\theta)\|^2 \geq 2\mu \left(\mathcal{L}(\theta) - \mathcal{L}^*\right)$$

$\mathcal{L}^*$ is the minimum achievable loss. $\mu > 0$ is the PL constant — how tight the inequality is. large $\mu$ means gradient size is a very informative signal about how far you are from the minimum.

reading this inequality: if your current loss $\mathcal{L}(\theta)$ is far above the minimum ($\mathcal{L}(\theta) - \mathcal{L}^*$ is large), the right side is large, so the left side (gradient squared) must also be large. big gradient means gradient descent takes a big step, which is good. if your current loss is close to the minimum, both sides are small: small gradient, small step, slow descent.

under PL, gradient descent converges geometrically. each step multiplies the remaining suboptimality by $(1 - 2\eta\mu)$:

$$E[\mathcal{L}(\theta_t) - \mathcal{L}^*] \leq (1 - 2\eta\mu)^t (\mathcal{L}_0 - \mathcal{L}^*) + \frac{\eta\sigma^2}{2\mu}$$

first term: geometric decay at rate $(1-2\eta\mu)^t$. at step $t$, it's reduced by a factor of $(1-2\eta\mu)^t$ relative to the initial gap. second term: irreducible noise floor from mini-batch gradient noise. even with infinite steps, the loss won't go below $\frac{\eta\sigma^2}{2\mu}$ in expectation.

the noise floor $\frac{\eta\sigma^2}{2\mu}$ has an important property: if the gradient noise $\sigma^2$ is structured and doesn't factor into the expectation (non-separable, as in Lemma 2's reverse direction), then $\sigma^2$ is effectively larger, which raises the floor and requires more steps to reach any given threshold.

the PL condition is weaker than convexity. a convex function has only one minimum (globally optimal), and gradient descent is guaranteed to find it. PL only requires the function to behave nicely near its local minima — the gradient gives useful information about how close you are. many overparameterized neural networks satisfy PL locally near their global minima, which is what the CCA theorem uses.

### Statistical independence vs correlation: the critical difference

correlation is about whether two variables move together linearly. the Pearson correlation coefficient is:

$$\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}$$

it's between $-1$ and $+1$. $+1$ means perfect positive linear relationship. $-1$ means perfect negative linear relationship. $0$ means no linear relationship. note: no linear relationship, not no relationship.

independence is strictly stronger: $X \perp Y$ means knowing $X$ gives you literally no information about $Y$ in any way, linear or nonlinear. independent implies uncorrelated, but uncorrelated doesn't imply independent.

the blog uses this distinction precisely. the ANM noise independence $\varepsilon \perp X$ is a full independence condition. the Lemma 1 argument shows that reverse residuals remain correlated with $Y$ — specifically $\text{Cov}(R_\text{rev}, Y) \neq 0$ — and this is enough to imply the factorization failure in Lemma 2.

why does correlation survive even after fitting? imagine fitting a regression of $X$ on $Y$ where the true relationship is nonlinear and non-injective. the network learns a function $h_\phi(Y)$, but because it can't perfectly recover $E[X \mid Y]$ with finite capacity, the residuals $X - h_\phi(Y)$ aren't just noise. they're correlated with $Y$ because the approximation error $\delta_\phi(Y) = h_\phi(Y) - E[X \mid Y]$ is a function of $Y$. that function of $Y$ is correlated with $Y$. so the residual is correlated with $Y$. full stop.



### Kolmogorov complexity and minimum description length

Kolmogorov complexity $K(x)$ is the length of the shortest program that outputs string $x$ when run on a universal Turing machine (a mathematical model of a general-purpose computer). it's a measure of the information content or compressibility of $x$. a string of a billion zeros has very low $K$ — the program is just "print 0 one billion times." a random string of a billion bits has $K$ close to a billion — no shorter program exists.

$K(x)$ is uncomputable: there's no algorithm that can take any string $x$ and return its Kolmogorov complexity. this is related to the halting problem. but it serves as a theoretical foundation for information theory and the idea that "simpler explanations are better."

Minimum Description Length (MDL), introduced by Rissanen in 1978, is the practical approximation. prefer the model $M$ that minimizes $\text{MDL}(M) = |M| + |D \mid M|$: how many bits to describe the model, plus how many bits to describe the data given the model. a more complex model describes itself in more bits but compresses the data more. simpler models use fewer bits to describe themselves but might not compress the data as well. the optimal MDL balances both.

for Gaussian linear models, Rissanen showed MDL is approximately equivalent to the Bayesian Information Criterion: $\text{BIC} = -2\log \hat{L} + k \log n$ where $\hat{L}$ is the maximized likelihood, $k$ is the number of parameters, and $n$ is the sample size. so MDL in the CCL framework is practical to compute using standard likelihood-based methods.

the deep reason MDL connects to causality: the Algorithmic Markov Condition (Janzing and Schölkopf 2010) says the true causal factorization of a joint distribution $P(X, Y)$ has shorter total Kolmogorov complexity than the anticausal factorization. in the true causal direction $X \to Y$, the cause distribution $P(X)$ and the mechanism $P(Y \mid X)$ are "chosen independently by nature" — neither knows about the other. independent things compress independently, and two independent short descriptions are shorter than one entangled long description. in the reverse direction, $P(X \mid Y)$ has to describe how to invert a noisy nonlinear function, which is inherently more complex than the forward mechanism. MDL naturally prefers the causal direction.



### Directed acyclic graphs and d-separation

a directed acyclic graph (DAG) is a set of nodes (variables) and directed edges (arrows between them) with no cycles. "acyclic" means you can't follow arrows and return to where you started. in causal modeling, each node is a variable, and an arrow $X \to Y$ means $X$ is a direct cause of $Y$ in the model.

d-separation is the graphical rule for reading off conditional independence from a DAG. two nodes $A$ and $B$ are d-separated by a set $C$ if every path between $A$ and $B$ is "blocked" by $C$. a path is a sequence of nodes connected by edges (ignoring direction). it can be blocked in two ways:

**non-collider blocking**: if a node on the path is a non-collider (either $\leftarrow Z \rightarrow$, meaning $Z$ causes both sides, or $\rightarrow Z \rightarrow$, meaning $Z$ is a mediator) and $Z \in C$, the path is blocked.

**collider blocking**: if a node is a collider ($\rightarrow Z \leftarrow$, meaning both sides cause $Z$) and neither $Z$ nor any of its descendants is in $C$, the path is blocked.

if a path is blocked, conditioning on $C$ doesn't transmit information along that path. d-separation $\implies$ conditional independence (under the Markov condition). faithfulness says the converse: conditional independence $\implies$ d-separation.

a simple example: $X \to Z \to Y$. $X$ and $Y$ are d-connected given empty set (there's an open path). condition on $Z$ (the mediator): the path $X \to Z \to Y$ is now blocked (non-collider $Z$ is in the conditioning set). so $X \perp Y \mid Z$.

another example: $X \leftarrow Z \rightarrow Y$ (common cause, confounding). $X$ and $Y$ are d-connected unconditionally (path through $Z$). condition on $Z$: path blocked. $X \perp Y \mid Z$.

collider example: $X \rightarrow Z \leftarrow Y$. unconditionally: path is blocked at the collider $Z$ (neither $Z$ nor descendants in conditioning set). $X \perp Y$. condition on $Z$: collider opens! $X$ and $Y$ become dependent given $Z$ — this is why conditioning on a collider creates spurious associations. it's called collider bias or Berkson's paradox.

understanding d-separation is what lets you read off every testable conditional independence from a causal graph, and it's the mathematical foundation for graph learning algorithms like PC-stable that recover causal graphs from data.


### What confounding is and why it wrecks observational studies

a confounder is a variable that causes both the treatment and the outcome, creating a spurious association between them. the classic example: a study finds that people who carry lighters are more likely to get lung cancer. lighters don't cause cancer. smoking causes both carrying a lighter and getting cancer. smoking is the confounder.

the mathematical picture: you're interested in $P(Y \mid \mathrm{do}(X))$. you compute $P(Y \mid X)$ instead. these are different because:

$$P(Y \mid X = x) = \int P(Y \mid X = x, Z = z) P(Z = z \mid X = x)\, dz$$

when there's a confounder $Z$, the distribution of $Z$ given $X$ is not the same as the distribution of $Z$ in the general population: $P(Z \mid X = x) \neq P(Z)$. people who smoke more also have different stress levels, income levels, diet, and so on. so the distribution of $Y$ given $X = x$ mixes the direct causal effect of $X$ with the associations introduced by $Z$.

the backdoor adjustment formula (from Pearl's do-calculus) says that if you can measure the confounder $Z$ and block all backdoor paths from $X$ to $Y$ (paths that go through variables that cause $X$):

$$P(Y \mid \mathrm{do}(X = x)) = \int P(Y \mid X = x, Z = z) P(Z = z)\, dz$$

you condition on $Z$ to control for confounding, but you weight by the marginal $P(Z)$ rather than $P(Z \mid X = x)$. this removes the spurious association. but this only works when you've measured and conditioned on all confounders — hidden confounders you haven't measured remain a problem.

the Causal IB in the CCL framework uses the do-operator precisely to remove confounding: $I_c(Y \mid \mathrm{do}(T))$ measures only the information $T$ carries about $Y$ through direct causal paths, because the do-operator removes any association through unmeasured common causes.


### What information entropy is

Shannon entropy $H(X)$ measures uncertainty about a random variable. for a discrete variable:

$$H(X) = -\sum_k P(X = k) \log P(X = k)$$

fair coin: $H = -0.5\log 0.5 - 0.5\log 0.5 = \log 2 \approx 0.693$ nats (using natural log) or 1 bit (using log base 2). maximum uncertainty. biased coin with $P(\text{heads}) = 0.9$: $H = -0.9\log 0.9 - 0.1\log 0.1 \approx 0.325$ nats. less uncertainty because you know heads is much more likely. a deterministic coin ($P(\text{heads}) = 1$): $H = 0$. no uncertainty at all.

entropy is maximized when all outcomes are equally likely and minimized (at zero) when one outcome has probability 1. higher entropy = more uncertainty = more information needed to describe a random draw.

mutual information $I(X; Y)$ measures how much knowing $Y$ reduces your uncertainty about $X$ (and vice versa — it's symmetric):

$$I(X; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)$$

$H(X \mid Y) = E_Y[H(X \mid Y = y)]$ is the conditional entropy: the average remaining uncertainty about $X$ after you know $Y$. $I(X; Y) = 0$ iff $X$ and $Y$ are independent (knowing $Y$ doesn't reduce uncertainty about $X$ at all). $I(X; Y) = H(X)$ iff knowing $Y$ tells you everything about $X$ (no residual uncertainty).

the formula in terms of distributions:

$$I(X; Y) = \int\!\!\int p(x, y) \log \frac{p(x, y)}{p(x)p(y)}\, dx\, dy$$

this is the KL divergence $D_\text{KL}(p(x,y) \| p(x)p(y))$ between the joint distribution and the product of marginals. it's zero iff the joint equals the product of marginals iff $X$ and $Y$ are independent. it's always $\geq 0$.

<img width="1280" height="903" alt="image" src="https://github.com/user-attachments/assets/0903d534-22ad-4268-a958-d8079d6682ee" />


the information bottleneck tradeoff $\min_T I(X;T) - \beta I(T;Y)$ makes sense in this language: $I(X;T)$ is how much information from $X$ you keep in the compressed representation $T$. you want this small (compress aggressively). $I(T;Y)$ is how much information $T$ contains about $Y$. you want this large (preserve predictive power). $\beta$ controls which matters more.



### What a covariance matrix is (brief)

when you have multiple variables $X_1, X_2, \ldots, X_p$, their pairwise covariances form a matrix:

$$\Sigma_{ij} = \text{Cov}(X_i, X_j)$$

the diagonal entries $\Sigma_{ii} = \text{Var}(X_i)$ are the variances. the off-diagonal entries measure pairwise co-movement. the covariance matrix is symmetric ($\Sigma_{ij} = \Sigma_{ji}$) and positive semidefinite (all eigenvalues $\geq 0$, which means the matrix can't assign negative variance to any linear combination of the variables).

diagonal covariance means all off-diagonal entries are zero: the variables are uncorrelated with each other. the VAE encoder in CCL uses diagonal Gaussian posterior covariance, which means the different components of the latent vector $T$ are modeled as uncorrelated given the input. this prevents the encoder from learning a single entangled direction that mixes two different causal parents, which would destroy the faithfulness of the representation.


### Mean squared error as a loss: why squaring, why not absolute value

you could measure prediction error as $|g_\theta(X) - Y|$ (absolute value, also called L1 loss) instead of $(g_\theta(X) - Y)^2$ (MSE, L2 loss). the key difference:

MSE penalizes large errors much more heavily than small errors (squaring amplifies large gaps). L1 treats all errors proportionally. for the CCA theory, MSE is natural because: (1) the optimal MSE minimizer is $E[Y \mid X]$, the conditional mean, which is a clean target. (2) the law of total variance decomposes MSE exactly into signal and noise components. (3) the gradient of MSE is a linear function of the residual, making optimization analysis tractable.

the minimum achievable MSE is $\text{Var}(Y \mid X) = E[(Y - E[Y \mid X])^2]$ — the average conditional variance. after z-scoring, this is the fraction of variance not explained by $X$. for the forward direction with $Y = f(X) + \varepsilon$, this is $E[\varepsilon^2] = \sigma_\varepsilon^2$ (since $f(X)$ is perfectly predictable from $X$). for the reverse direction, it's $E[\text{Var}(X \mid Y)]$, which is in general different and larger (as Lemma 2 shows).


### What z-scoring does mathematically

z-scoring (standardizing) a variable $X$ means subtracting its mean and dividing by its standard deviation:

$$\tilde{X} = \frac{X - \mu_X}{\sigma_X}$$

after this transformation: $E[\tilde{X}] = 0$ (the mean is zero) and $\text{Var}(\tilde{X}) = 1$ (the variance is one). this makes $\tilde{X}$ dimensionless and scale-free.

why does this matter for CCA? the convergence threshold $\tau = 0.05$ is set in units of the prediction target's variance. if both $X$ and $Y$ are z-scored to unit variance, then $\tau = 0.05$ means "predict 95% of the variance" for both networks. the comparison is apples-to-apples. without z-scoring, if $\text{Var}(Y) = 15$ and $\text{Var}(X) = 1$, then $\tau = 0.05$ means "predict 99.7% of $Y$'s variance" for the forward network and "predict 95% of $X$'s variance" for the reverse network. the reverse network has a much easier task and will converge first even when $X$ is the cause — a spurious result from scale mismatch.

z-scoring is also the reason the blog computes $E[X^6]$ for $X \sim \mathcal{N}(0,1)$: to find $\text{Var}(X^3) = E[X^6] - (E[X^3])^2 = 15 - 0 = 15$, which determines how different the scales of the forward and reverse problems are without z-scoring.


### What the Gaussian moments are and how to compute them

for $X \sim \mathcal{N}(0, 1)$, the moments $E[X^k]$ follow a pattern. odd moments are zero by symmetry of the distribution around zero: $E[X] = E[X^3] = E[X^5] = 0$. even moments use the formula:

$$E[X^{2k}] = (2k-1)!! = (2k-1) \times (2k-3) \times \ldots \times 3 \times 1$$

where $!!$ means "double factorial." so:
- $E[X^2] = 1$
- $E[X^4] = 3!! = 3$
- $E[X^6] = 5!! = 5 \times 3 \times 1 = 15$
- $E[X^8] = 7!! = 7 \times 5 \times 3 \times 1 = 105$

the result $E[X^6] = 15$ is used in the z-scoring section to show that without standardization, $\text{Var}(X^3) = E[X^6] - (E[X^3])^2 = 15 - 0 = 15$. this means $Y = X^3 + \varepsilon$ has variance about 15 times larger than $X$ for small noise, which creates the scale asymmetry that wrecks CCA without z-scoring.

these moments come from a general formula using Wick's theorem (or the Isserlis theorem) from physics: for a Gaussian, every even moment factors into sums of products of pairwise covariances. for $X^6$ with unit variance, there are $5!! = 15$ ways to partition $\{1,2,3,4,5,6\}$ into three pairs, each contributing $E[X^2]^3 = 1$, giving $E[X^6] = 15$.


### Lipschitz functions: why they matter

a function $f : \mathbb{R} \to \mathbb{R}$ is Lipschitz continuous with constant $L$ if:

$$|f(x) - f(x')| \leq L |x - x'| \quad \text{for all } x, x'$$

geometrically: the slope of $f$ never exceeds $L$ anywhere. the function can't change faster than $L$ units of output per unit of input. $L = 1$ means slopes bounded by 1. $L = 10$ means slopes bounded by 10.

$f(x) = x^2$ is not globally Lipschitz (slope $= |2x|$ grows unbounded as $x \to \infty$), but it is Lipschitz on any bounded interval.

$f(x) = \sin(x)$ is globally Lipschitz with $L = 1$ (the derivative $\cos(x)$ never exceeds 1 in absolute value).

why does this matter for the proofs? the Lipschitz condition on $f$ bounds how fast the forward regression target $f(X)$ can change as $X$ changes. this in turn bounds how fast the gradient of the forward loss changes as parameters change, which is needed to establish that the PL condition holds with a specific constant $\mu$. it also bounds the variance of gradient estimates — if $f$ has bounded slope, the gradient of the loss at each data point doesn't blow up for large $X$.


### Random variables and probability distributions

a random variable is just a quantity whose value is determined by some random process. roll a die: the outcome is a random variable. measure the height of a random person: that's a random variable. record whether it rains tomorrow: another random variable.

mathematically, a random variable $X$ is a function that maps outcomes of a random experiment to numbers. the die has six outcomes $\{1, 2, 3, 4, 5, 6\}$ each with probability $1/6$. the height of a person has a continuous range of possible values with some probability density over them.

a probability distribution describes how likely each value is. for discrete variables (like a die), you can just list $P(X = k)$ for each possible $k$. for continuous variables (like height), you work with a probability density function $p(x)$, which satisfies two things: $p(x) \geq 0$ everywhere, and $\int_{-\infty}^{\infty} p(x)\, dx = 1$. the probability of landing in some range $[a, b]$ is $\int_a^b p(x)\, dx$. a single point has probability zero for continuous variables.

the most important continuous distribution is the Gaussian (normal) distribution. its density is:

$$p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\!\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$$

$\mu$ is the center (the mean, where the bell peaks) and $\sigma^2$ is the spread (how wide the bell is). the notation $X \sim \mathcal{N}(\mu, \sigma^2)$ means "$X$ is drawn from a Gaussian with mean $\mu$ and variance $\sigma^2$." a standard normal is $\mathcal{N}(0, 1)$: centered at zero, spread of one.

> *[image suggestion: two bell curves side by side. left: $\mathcal{N}(0, 1)$ — narrow, tall, centered at zero. right: $\mathcal{N}(0, 4)$ — wider, shorter, also centered at zero. annotate with $\mu = 0$ and $\sigma^2 = 1$ vs $\sigma^2 = 4$. these should look like physical bell curves with the x-axis running from about $-4$ to $4$. make obvious that $\sigma^2$ controls width.]*

why Gaussian? the Central Limit Theorem says that the average of many independent random quantities, no matter what their individual distributions look like, will tend toward a Gaussian as you average more of them. noise in real measurements is usually many small random perturbations summed up, so Gaussian noise is physically realistic.


### Expected value, variance, and covariance

the expected value $E[X]$ is the probability-weighted average of all possible values of $X$. for a discrete variable: $E[X] = \sum_k k \cdot P(X = k)$. for a continuous variable: $E[X] = \int_{-\infty}^{\infty} x \cdot p(x)\, dx$. intuitively: if you ran the random experiment a huge number of times and averaged all the outcomes, you'd converge to $E[X]$.

expected value is linear. $E[aX + b] = aE[X] + b$ for any constants $a$ and $b$. and $E[X + Y] = E[X] + E[Y]$ whether or not $X$ and $Y$ are independent. this linearity is constantly used in the proofs in the main blog.

variance measures how spread out the values of $X$ are around its expected value. the formula is:

$$\text{Var}(X) = E\!\left[(X - E[X])^2\right]$$

take every possible value of $X$, subtract the mean, square the difference (so negatives don't cancel positives), weight by probability, and sum. the result is always $\geq 0$. zero variance means $X$ is a constant. high variance means values are spread far from the mean.

a useful identity that comes up constantly: $\text{Var}(X) = E[X^2] - (E[X])^2$. proved by expanding $(X - E[X])^2 = X^2 - 2X\cdot E[X] + (E[X])^2$, taking expectations, and using linearity.

the standard deviation $\sigma_X = \sqrt{\text{Var}(X)}$ is just variance in the original units of $X$ rather than squared units. if $X$ is in meters, variance is in meters squared, standard deviation is in meters.

covariance measures how two variables $X$ and $Y$ move together:

$$\text{Cov}(X, Y) = E\!\left[(X - E[X])(Y - E[Y])\right]$$

if $X$ tends to be above its mean when $Y$ is above its mean (they move together), covariance is positive. if $X$ above average tends to go with $Y$ below average (they move opposite), covariance is negative. if knowing where $X$ is gives no information about where $Y$ is, covariance is zero.

an equivalent formula: $\text{Cov}(X, Y) = E[XY] - E[X]E[Y]$. this comes up directly in Lemma 1's proof. independent variables have $E[XY] = E[X]E[Y]$, so $\text{Cov}(X, Y) = 0$ when $X \perp Y$.

the law of total variance is used explicitly in Lemma 2:

$$\text{Var}(X) = E[\text{Var}(X \mid Y)] + \text{Var}(E[X \mid Y])$$

this says: the total variance of $X$ decomposes into two parts. the first, $E[\text{Var}(X \mid Y)]$, is the variance that remains within each value of $Y$ — on average, how uncertain are you about $X$ even after you know $Y$. the second, $\text{Var}(E[X \mid Y])$, is the variance of the conditional mean across different values of $Y$ — how much does knowing $Y$ shift your prediction of $X$. these two parts sum to the total variance. the blog uses this to argue that $E[\text{Var}(X \mid Y)] = \text{Var}(X) - \text{Var}(E[X \mid Y]) < \text{Var}(X)$, which proves the reverse network's minimum loss is strictly less than 1 after z-scoring.

> *[image suggestion: a visual breakdown of law of total variance using a scatter plot of $(X, Y)$ pairs from a nonlinear relationship like $Y = X^2 + \varepsilon$. draw vertical slices at different $Y$ values. each slice has its own conditional distribution of $X$ — show a small density curve within each slice. the width of each slice's distribution is $\text{Var}(X \mid Y=y)$. the movement of the center of each slice as $Y$ changes is $E[X \mid Y=y]$. label the two components of the law of total variance.]*


### What independence means

two random variables $X$ and $Y$ are independent if knowing the value of one gives you zero information about the other. formally: $P(X = x, Y = y) = P(X = x) \cdot P(Y = y)$ for all $x$ and $y$ (or the density version $p(x, y) = p(x) \cdot p(y)$). the joint distribution factors into the product of the marginals.

this is stricter than zero covariance. $\text{Cov}(X, Y) = 0$ means no linear relationship. but $X$ and $Y$ could have $\text{Cov} = 0$ while still being dependent in a nonlinear way. for example, let $X \sim \mathcal{N}(0,1)$ and $Y = X^2$. then $E[X] = 0$ and $E[XY] = E[X \cdot X^2] = E[X^3] = 0$ (odd moment of symmetric distribution), so $\text{Cov}(X, Y) = 0$. but $X$ and $Y$ are clearly not independent: knowing $Y = 4$ tells you $X = \pm 2$. independence requires the whole joint distribution to factor, not just linear correlations to vanish.

independence implies $\text{Cov}(X, Y) = 0$, but not the other way around. the ANM uses noise independence $\varepsilon \perp X$ in the stronger sense: the full joint distribution factors. this is why the forward residuals becoming uncorrelated with $X$ as the network approaches $f$ is a clean convergence story — uncorrelated noise that is also independent means truly structureless noise.


### Conditional expectation and the tower property

$E[X \mid Y = y]$ is the expected value of $X$ given that $Y$ takes the specific value $y$. it's a function of $y$: for each value $y$ can take, you compute the average of $X$ restricted to observations where $Y = y$.

the formula for continuous distributions: $E[X \mid Y = y] = \int x \cdot p(x \mid Y = y)\, dx$ where $p(x \mid Y = y) = p(x, y) / p(y)$ is the conditional density.

$E[X \mid Y]$ (without fixing $y$) is the random variable you get by plugging in the actual random $Y$: for each realization of $Y$, it gives the expected value of $X$ at that $Y$. this is itself a random variable.

the tower property (also called the law of iterated expectations): $E[E[X \mid Y]] = E[X]$. averaging the conditional expectation over all values of $Y$ recovers the unconditional expectation. proof: $E[E[X \mid Y]] = \int \left(\int x \cdot p(x \mid y)\, dx\right) p(y)\, dy = \int \int x \cdot p(x, y)\, dx\, dy = \int x \cdot p(x)\, dx = E[X]$.

the key consequence used in Lemma 1: $E\left[g(Y) \cdot (X - E[X \mid Y])\right] = 0$ for any function $g$. this is because $E[g(Y) \cdot (X - E[X \mid Y]) \mid Y = y] = g(y) \cdot E[X - E[X \mid Y] \mid Y = y] = g(y) \cdot (E[X \mid Y = y] - E[X \mid Y = y]) = 0$. taking expectations over $Y$ gives zero. this means the optimal reverse predictor $h^*(Y) = E[X \mid Y]$ has residuals $X - h^*(Y)$ that are uncorrelated with any function of $Y$ — in particular uncorrelated with $Y$ itself.

> *[image suggestion: two scatter plots side by side from a simulated dataset of $(X, Y)$ pairs where $Y = \sin(X) + \varepsilon$. left: raw scatter, $X$ on x-axis, $Y$ on y-axis, showing the sine curve with noise. right: $Y$ on x-axis, $X$ on y-axis (the reverse problem). overlay the true $E[X \mid Y]$ curve on the right panel — it should look like a wiggly average, not a clean function, because sine is not injective over its full domain and the conditional mean averages over multiple branches. this makes "conditional expectation as a function of $Y$" visually concrete.]*


### Derivatives, partial derivatives, and gradients

a derivative tells you the rate of change of a function. if $f(x) = x^2$, then $f'(x) = 2x$: the function is changing at rate $2x$ when the input is $x$. at $x = 3$, the function is increasing at rate 6. at $x = 0$, the rate of change is 0 (it's flat at the bottom).

for a function of multiple inputs, say $L(w_1, w_2)$ where $w_1$ and $w_2$ are two weights in a neural network, a partial derivative $\frac{\partial L}{\partial w_1}$ tells you the rate of change with respect to $w_1$ while holding $w_2$ fixed. it's just a regular derivative treating $w_2$ as a constant.

the gradient $\nabla L$ is the vector of all partial derivatives:

$$\nabla L(w_1, w_2) = \left(\frac{\partial L}{\partial w_1},\, \frac{\partial L}{\partial w_2}\right)$$

it points in the direction of steepest increase of $L$. if you want to minimize $L$ (which is what training does — minimize the loss), you move in the direction of $-\nabla L$, the negative gradient.

for a neural network with parameters $\theta = (\theta_1, \theta_2, \ldots, \theta_n)$ (potentially millions of numbers), the gradient $\nabla_\theta \mathcal{L}$ is a vector of the same dimension as $\theta$. each component tells you how the loss changes if you nudge that parameter slightly. gradient descent updates each parameter by stepping in the direction that reduces loss.

the gradient norm $\|\nabla_\theta \mathcal{L}\|$ is just the length of the gradient vector: $\sqrt{\sum_i (\partial \mathcal{L} / \partial \theta_i)^2}$. large gradient norm means the loss is changing steeply with respect to parameters — you're far from flat ground. small gradient norm means you're near a local minimum. exactly zero gradient norm means you're at a critical point (minimum, maximum, or saddle point).

the Jacobian is the matrix-valued generalization: for a vector-valued function $g : \mathbb{R}^n \to \mathbb{R}^m$, the Jacobian $J$ is the $m \times n$ matrix of all partial derivatives $J_{ij} = \partial g_i / \partial x_j$. for a scalar-output network $g_\theta : \mathbb{R} \to \mathbb{R}$, the Jacobian is just the gradient vector $\nabla_\theta g_\theta$. the Jacobian norm $\|J\|$ measures how much a small change in the input stretches the output. large Jacobian norms appear in Lemma 2 when explaining why the forward network learning $x \mapsto x^3$ has large gradient variances: $x^3$ is steep, so small changes in $\theta$ cause large changes in outputs.

## what an additive noise model actually is and why it's the right framework

CCA operates within the Additive Noise Model (ANM) framework. the assumed data generating process is:

$$Y = f(X) + \varepsilon$$

$Y$ is determined by some function $f$ applied to $X$, plus noise $\varepsilon$ that is statistically independent of $X$. that's the whole generative model. two variables, one mechanism, one noise term.

this framework has a history. Hoyer, Janzing, Mooij, Peters, and Schölkopf introduced nonlinear ANMs at NeurIPS 2008 and showed that for nonlinear $f$, generic models generate joint distributions that don't admit an ANM in the reverse direction. Peters, Mooij, Janzing, and Schölkopf then gave the full formal identifiability proof in JMLR 2014 (the paper that RESIT comes from). the key result: under the ANM with nonlinear $f$ and noise $\varepsilon \perp X$, you almost always cannot write an equivalent ANM in the other direction. $Y \to X$ with independent noise is almost impossible if $X \to Y$ with independent noise is the true model.

the CCA framework requires three conditions on this ANM. first, $f$ must be nonlinear. second, $f$ must be Lipschitz continuous, meaning $|f(x) - f(x')| \leq L_f |x - x'|$ for some constant $L_f$ — the function can't change infinitely fast anywhere. third and most importantly, $f$ must be injective, meaning one-to-one: $f(x_1) = f(x_2) \implies x_1 = x_2$. distinct inputs always produce distinct outputs.

the noise independence condition $\varepsilon \perp X$ is what encodes the causal direction as a probabilistic property. in the true causal direction, the noise that perturbs $Y$ comes from the world independently of what $X$ is doing. mechanism $f$ operates on $X$, random variation $\varepsilon$ gets added on top, and $\varepsilon$ has no idea what $X$ was. if you try to write the reverse as $X = g(Y) + \eta$ for some function $g$ and noise $\eta$, you cannot achieve $\eta \perp Y$. because $Y = f(X) + \varepsilon$ has already entangled $X$ and $\varepsilon$ inside $Y$. untangling them in reverse is structurally impossible. this asymmetry between forward and reverse is the entire basis of the method.

to see why injectivity matters so profoundly, think about $Y = X^2 + \varepsilon$ with $X \sim \mathcal{N}(0,1)$. $f(x) = x^2$ is not injective: $f(1) = f(-1) = 1$. what is $E[X \mid Y = y]$? since $P(X)$ is symmetric around zero and $f$ maps both $x = 1$ and $x = -1$ to the same $f$-value, by symmetry:

$$E[X \mid Y = y] = 0 \quad \text{for all } y$$

the optimal reverse predictor is just the constant function zero. a neural network learning to predict $X$ from $Y$ will converge to predicting zero in about ten training steps. because gradient descent starting near zero for an MSE loss, where the true target is zero, will just immediately jump to the minimum. the forward network meanwhile needs hundreds of steps to learn $x \mapsto x^2$. CCA sees forward takes 1000 steps, reverse takes 8, scores hugely positive, predicts $Y \to X$. wrong. every single seed. this is boundary condition 2 and it's a completely predicted failure.


why does Lipschitz matter? it bounds how fast $f$ can change and is needed in the proofs of Lemma 2 and the main theorem. specifically, the gradient of the forward network's loss with respect to parameters involves $f'(x)$ (the derivative of the mechanism), and bounding the Lipschitz constant of $f$ bounds how large $\|f'(x)\|$ can be, which controls the variance of gradient estimates.

the full formal setup sits inside a Structural Causal Model (SCM), a mathematical object introduced by Pearl. an SCM is a tuple $\mathcal{M} = \langle U, V, F, P(U) \rangle$ where $U$ are exogenous (background) variables — the noise sources of the world — $V$ are the observed variables, $F = \{f_i\}$ are the structural equations where each variable is determined by its causal parents plus noise, and $P(U)$ is typically assumed to have independent components (each variable gets its own independent noise). the causal graph $G$ is the DAG where you draw an arrow $X_i \to X_j$ whenever $X_i$ appears in $X_j$'s equation. the Markov condition says each variable is independent of its non-descendants given its causal parents. faithfulness says every conditional independence in the data is entailed by a d-separation in $G$ — no accidental cancellations.

d-separation is worth understanding. given a DAG, two sets of nodes $A$ and $B$ are d-separated by $C$ if every path between them is blocked. a path is blocked at a non-collider node in $C$ (a node where arrows don't collide), or at a collider node not in $C$ (a node where two arrows point into it). under faithfulness, d-separation in $G$ exactly characterizes conditional independence in $P(V)$. this is what lets you read off testable implications of a causal graph from its topology.

for the bivariate ANM: $V = \{X, Y\}$, $U = \{X, \varepsilon\}$ (treating $X$ as exogenous), and the single structural equation is $Y = f(X) + \varepsilon$. the causal graph is just one node and one arrow: $X \to Y$. CCA's job is to infer, from observational data $(X_i, Y_i)$, that this is the correct direction and not $Y \to X$.


## what existed before CCA and exactly where each method breaks

three major families of methods existed. understanding where they each fail helps place the new idea.

**RESIT (regression with subsequent independence test), Peters et al. 2014.** the idea: under the true ANM $Y = f(X) + \varepsilon$ with $\varepsilon \perp X$, fit a nonlinear regression of $Y$ on $X$ and test whether the residuals $\hat{\varepsilon}_i = Y_i - \hat{f}(X_i)$ are independent of $X$. in the true causal direction, residuals converge to $\varepsilon$ which is independent of $X$ by assumption. in the reverse direction, residuals carry systematic structure. the method tests independence (using HSIC, the Hilbert-Schmidt Independence Criterion, which measures dependence via kernel methods) in both directions and picks the direction where independence holds.

where it breaks: non-injective $f$. Peters et al. proved this themselves in Proposition 23 of their paper. when $f$ is not injective, both the forward and reverse residuals can end up approximately independent of the input by symmetry, so the test gives no information. on the Tübingen cause-effect pairs benchmark — 108 real-world variable pairs with known ground truth — RESIT achieves 63% accuracy. the majority-class baseline (always predict the more common direction) is 72.2%. RESIT is actively worse than the trivial baseline on real data. this isn't a knock on the researchers; real mechanisms aren't always clean injective nonlinear functions.

**IGCI (information geometric causal inference), Janzing and Schölkopf 2010.** this one goes to the algorithmic complexity level. the Algorithmic Markov Condition says: the true causal factorization of a joint distribution has shorter total Kolmogorov complexity than the anticausal factorization. Kolmogorov complexity $K(x)$ is the length of the shortest program on a universal Turing machine that outputs $x$. the idea is that in the true causal direction, the cause distribution $P(X)$ and the mechanism $P(Y \mid X)$ are "chosen independently by nature" — the input distribution and the causal mechanism don't know about each other, so they compress separately and the total description is shorter. in the reverse direction, $P(X \mid Y)$ must describe how to invert a noisy nonlinear function, which requires knowing both the forward mechanism and the noise distribution jointly. the result is a longer description. IGCI approximates Kolmogorov complexity (which is uncomputable) using entropy-based proxies and scores pairs by which direction has shorter description. in practice the signal can be weak for many functional forms and the approximation quality varies. IGCI achieves roughly 60% on Tübingen.

**SkewScore, Lin et al. 2025 (ICLR).** this uses the score function $s(x) = \nabla_x \log p(x)$ — the gradient of the log-density — and its skewness as a directional signal. for heteroscedastic ANMs where noise magnitude depends on $X$ (i.e., $Y = f(X) + \sigma(X)\varepsilon$), the score function of the anticausal distribution has detectably different skewness properties than the causal one. SkewScore is specifically designed for heteroscedastic noise and handles cases RESIT can't. it operates on distributional statistics of the score function.

CCA uses none of these. it doesn't test residual independence. it doesn't measure description length. it doesn't analyze score function moments. it measures optimization convergence time: how many gradient descent steps does a neural network need to reach a target validation loss. this is a different mathematical space, and the paper proves from first principles why a directional signal exists there.


## the three lemmas and the asymmetry theorem

to understand why convergence time encodes causal direction, you need to follow the proof chain. three lemmas, each one building on the last.

**lemma 1: reverse residuals stay correlated with the input, forever, for any finite-capacity approximation.**

in the forward direction, the network $g_\theta$ tries to learn $Y$ from $X$. the true target is $f(X)$ plus irreducible noise $\varepsilon$. as the network approaches $f$, the residuals $R_\text{fwd} = Y - g_\theta(X) = f(X) + \varepsilon - g_\theta(X) \to \varepsilon$. and $\varepsilon \perp X$ by the ANM assumption. forward residuals decorrelate from the input as training progresses.

in the reverse direction, the network $h_\phi$ tries to learn $X$ from $Y$. the optimal reverse predictor (the thing that minimizes MSE) is the conditional expectation $h^*(Y) = E[X \mid Y]$. for any finite-capacity approximation $h_\phi \neq h^*$, define the approximation error $\delta_\phi(Y) = h_\phi(Y) - h^*(Y)$. the residual decomposes as:

$$R_\text{rev} = X - h_\phi(Y) = \underbrace{(X - h^*(Y))}_{R^*} - \delta_\phi(Y)$$

at the population optimum, $R^* = X - E[X \mid Y]$ satisfies $E[R^* \mid Y] = 0$ by the tower property of conditional expectation (which says $E[E[X \mid Y]] = E[X]$ and more generally $E[g(Y) \cdot (X - E[X \mid Y])] = 0$ for any function $g$). so $\text{Cov}(R^*, Y) = 0$. now the covariance of the actual residual:

$$\text{Cov}(R_\text{rev}, Y) = \text{Cov}(R^*, Y) - \text{Cov}(\delta_\phi(Y), Y) = 0 - \text{Cov}(\delta_\phi(Y), Y)$$

since $h_\phi \neq h^*$, $\delta_\phi(Y)$ is a non-constant function of $Y$. under the full-support condition on $P(X)$, any non-constant measurable function of $Y$ has nonzero covariance with $Y$. therefore $\text{Cov}(R_\text{rev}, Y) \neq 0$ for any finite approximation. this is not a convergence failure. this is a permanent mathematical property of the problem. you cannot train your way out of it. the correlation is baked into the geometry of the reverse regression problem.

to make this concrete: in the forward case, as the network learns $f$, residuals become cleaner and cleaner noise, uncorrelated with $X$. in the reverse case, residuals stay systematically structured relative to $Y$. the optimizer gets cleaner gradients in the forward direction. in reverse, every gradient step is fighting structured noise that depends on where you are in the input space.


**lemma 2: that persistent correlation does two specific things to the optimization landscape.**

the first effect is a higher population minimum loss. the minimum achievable MSE for the forward direction is just the variance of the irreducible noise:

$$\mathcal{L}^*_\text{fwd} = E[\varepsilon^2] = \sigma_\varepsilon^2$$

after z-scoring $Y$ so that $\text{Var}(Y) = 1$, we have $\text{Var}(f(X)) + \sigma_\varepsilon^2 = 1$, so $\sigma_\varepsilon^2 < 1$ (assuming $f$ is non-constant). the forward network can achieve MSE strictly less than 1.

the minimum achievable MSE for the reverse direction is:

$$\mathcal{L}^*_\text{rev} = E[\text{Var}(X \mid Y)]$$

to see where this comes from: the law of total variance says $\text{Var}(X) = E[\text{Var}(X \mid Y)] + \text{Var}(E[X \mid Y])$. after z-scoring $X$ so that $\text{Var}(X) = 1$, we get $E[\text{Var}(X \mid Y)] = 1 - \text{Var}(E[X \mid Y])$. since $X$ and $Y$ are dependent (they're connected by the ANM), $E[X \mid Y]$ is non-constant, so $\text{Var}(E[X \mid Y]) > 0$, so $\mathcal{L}^*_\text{rev} < 1$. both minima are below 1. but the question is which is smaller.

for nonlinear injective $f$, $\text{Var}(X \mid Y = y)$ varies with $y$. in regions where $f$ has high curvature, nearby $X$ values produce similar $Y$ values, so the conditional variance is high — knowing $Y$ doesn't narrow down $X$ much. in regions where $f$ has large derivative and is nearly monotone, each $Y$ value pins down $X$ precisely, so conditional variance is low. this heteroscedasticity means the reverse loss landscape has a non-uniform noise floor that varies across the input space. the optimal reverse network can't achieve uniformly small residuals everywhere simultaneously.

the second effect is non-separable gradient covariance. the variance of a mini-batch gradient estimate for the reverse network is approximately:

$$\text{Var}(\hat{\nabla}_\phi \mathcal{L}) \approx \frac{1}{m} E\!\left[(X - h_\phi(Y))^2 \cdot \|\nabla_\phi h_\phi(Y)\|^2\right]$$

for gradient noise to behave well — specifically, to decrease as $O(1/m)$ as batch size $m$ increases — this expectation needs to factorize:

$$E[\text{residual}^2 \cdot \text{gradient norm}^2] \approx E[\text{residual}^2] \cdot E[\text{gradient norm}^2]$$

factorization holds when the squared residual and squared gradient norm are uncorrelated. in the forward direction, as the network approaches $f$, residuals converge to $\varepsilon$ which is independent of $X$. the gradient $\nabla_\theta g_\theta(X)$ depends on $X$. but $\varepsilon \perp X$, so residual magnitude is uncorrelated with gradient magnitude, and factorization holds. batch gradient estimates average down nicely.

in the reverse direction, from Lemma 1, $R_\text{rev}$ is correlated with $Y$ throughout training. the gradient $\nabla_\phi h_\phi(Y)$ also depends on $Y$. so:

$$\text{Cov}\!\left(R_\text{rev}^2, \|\nabla_\phi h_\phi(Y)\|^2\right) \neq 0$$

factorization fails. the gradient noise is not reducible by increasing batch size past a certain point. it has a structural floor that persists regardless of how many samples you use per step. every gradient step in the reverse direction is fighting noise that's correlated with the error and correlated with the position in parameter space, which creates a feedback that systematically slows convergence.

there's a counterintuitive empirical finding worth noting. if you just measure instantaneous gradient norm variance at a single training step, the ratio (reverse / forward) for $Y = X^3$ (z-scored) is 0.376 at initialization and 0.054 at mid-training. both below 1. this seems to say the forward direction has larger gradient variance, which is backwards from the story. the explanation: the forward network is learning $x \mapsto x^3$, a function with large Jacobian norms ($\|f'(x)\|$ for cubic is $|3x^2|$ which can be large). the reverse network is fitting $E[X \mid Y] \approx 0$ (near-constant for symmetric input distribution), which has small Jacobian norms. so instantaneous gradient variance is dominated by Jacobian size, not by the residual correlation structure. for $Y = \sin(X)$, the ratio is 1.161 at initialization and grows to 2.179 at mid-training, because the reverse target $E[X \mid Y]$ for sine involves averaging over multiple branches of the inverse sine, giving a non-trivial function with large Jacobians in the reverse network. which mechanism dominates depends on the specific DGP. the correct proxy for Lemma 2's landscape difficulty is convergence time, not instantaneous gradient statistics.

<img width="840" height="435" alt="image" src="https://github.com/user-attachments/assets/b57a3e47-a71a-400e-9a9f-6fdfa3c91f19" />


**lemma 3: harder optimization landscape means more steps required.**

the Polyak-Łojasiewicz (PL) condition is a regularity condition on an optimization objective near a minimum. for a loss function $\mathcal{L}$ with minimum value $\mathcal{L}^*$, PL says:

$$\|\nabla \mathcal{L}(\theta)\|^2 \geq 2\mu \left(\mathcal{L}(\theta) - \mathcal{L}^*\right)$$

for some constant $\mu > 0$. the interpretation: how large the gradient is lower-bounds how far you are from the optimum. if you're far from the minimum ($\mathcal{L}(\theta) - \mathcal{L}^*$ is large), your gradient magnitude must be large. if your gradient is small, you must be near the minimum. the PL constant $\mu$ measures how tight this relationship is — larger $\mu$ means faster convergence because gradient size is more informative about suboptimality.

PL is strictly weaker than strong convexity. a strongly convex function satisfies PL, but many non-convex functions satisfy PL too. Polyak proved in 1963 that gradient descent achieves linear convergence under PL. Karimi et al. (2016) showed that PL is implied by strong convexity, quasi-strong convexity, essential strong convexity, weak strong convexity, and restricted secant inequality — it's actually the weakest condition that still gives linear convergence. for overparameterized neural networks, recent work (particularly Xu et al. 2025, and the local PL analysis of Aich et al. 2025) shows that PL holds locally near global minima with constants that depend on the network weights and initialization.

Under PL, gradient descent converges geometrically. Each step multiplies the remaining suboptimality by $(1 - 2\eta\mu)$:

$$
\mathbb{E}\left[\mathcal{L}(\theta_t) - \mathcal{L}^{*}\right]
\le
(1 - 2\eta\mu)^{t} (\mathcal{L}_0 - \mathcal{L}^{*})
+ \frac{\eta\sigma^2}{2\mu}.
$$

the first term is geometric decay at rate $(1-2\eta\mu)$. to drive it below $\varepsilon$ you need $t \geq \frac{1}{2\eta\mu}\log\frac{\mathcal{L}_0 - \mathcal{L}^*}{\varepsilon}$ steps. the second term $\frac{\eta\sigma^2}{2\mu}$ is an irreducible noise floor — no matter how many steps you run, expected loss can't go below this. decreasing learning rate decreases this floor but also slows the geometric decay rate in the first term. there's a fundamental tradeoff.

lemma 3 says two separate conditions apply in the reverse direction and both require more steps. condition (a): if the convergence threshold $\tau$ is between the two minimum losses (i.e., $\mathcal{L}^*_\text{fwd} < \tau < \mathcal{L}^*_\text{rev}$), the forward network can reach $\tau$ but the reverse network literally cannot. its noise floor is above $\tau$. infinite training won't help. $T_\text{fwd}$ is finite, $T_\text{rev} = T_\text{max}$ (hits the cap). condition (b): when $\tau$ is above both minima, the reverse network still needs more steps because: (1) it needs to close a larger effective gap, since $\mathcal{L}_0 - \tau$ is larger for reverse when $\mathcal{L}^*_\text{rev} > \mathcal{L}^*_\text{fwd}$, and (2) the non-separable gradient covariance inflates the effective $\sigma^2$ in the noise floor term, meaning the noise bias is larger and drives the optimizer away from threshold more persistently.

the step count gap satisfies at least:

$$E[T_\text{rev}] \geq E[T_\text{fwd}] + \Omega\!\left(\frac{1}{\eta\mu}\right)$$

inversely proportional to learning rate and PL constant. the actual empirical gap is much larger. on $Y = X^3 + \varepsilon$ with z-scoring, forward converges at step 161, reverse never converges within 3000 steps — a 19-fold gap. the theoretical bound says "a gap exists." saddle points and flat regions in the reverse landscape (which the local PL analysis doesn't capture) amplify it substantially.

**the CCA asymmetry theorem.** under the ANM with nonlinear injective $f$ and z-scoring, if both objectives satisfy PL locally near their minima:

$$E[T_\text{fwd}] < E[T_\text{rev}]$$

proof chain: Lemma 1 says reverse residuals stay correlated with $Y$. Lemma 2 says this correlation creates (i) a higher minimum loss for the reverse direction and (ii) non-separable gradient covariance, both making the reverse optimization harder. Lemma 3 says harder optimization requires more expected steps under PL. chain complete. $\square$

## z-scoring: the most important preprocessing step in the whole method

the z-scoring requirement sounds like a boring data hygiene note. it completely changes whether the method works.

z-scoring just means standardizing each variable to have mean 0 and standard deviation 1:

$$\tilde{X} = \frac{X - \mu_X}{\sigma_X}, \qquad \tilde{Y} = \frac{Y - \mu_Y}{\sigma_Y}$$

why does this matter? consider $Y = X^3 + \varepsilon$ with $X \sim \mathcal{N}(0,1)$. to compute $\text{Var}(X^3)$: we need $E[X^6]$. for a standard normal, there's a formula for all even moments: $E[X^{2k}] = (2k-1)!! = (2k-1) \times (2k-3) \times \ldots \times 3 \times 1$. so $E[X^6] = 5!! = 5 \times 3 \times 1 = 15$. therefore $\text{Var}(Y) = \text{Var}(X^3) + \sigma_\varepsilon^2 \approx 15 + \sigma_\varepsilon^2$.

now set convergence threshold $\tau = 0.05$ (5% of unit variance). the forward network predicts $Y$ in a space where $\text{Var}(Y) \approx 15$. to achieve MSE below 0.05, it needs to explain $\frac{15 - 0.05}{15} \approx 99.7\%$ of $Y$'s variance. the reverse network predicts $X$ where $\text{Var}(X) = 1$. to achieve MSE below 0.05, it needs to explain 95% of $X$'s variance. the reverse network is playing an asymmetrically easier game purely because of variance scale, not because of any causal structure. the scale asymmetry dominates.

| | without z-scoring | with z-scoring |
|---|---|---|
| mean $T_\text{fwd}$ (30 seeds) | $1152.5 \pm 904.0$ | $323 \pm 531$ |
| mean $T_\text{rev}$ (30 seeds) | $468.8 \pm 860.4$ | $717 \pm 789$ |
| correct direction predictions | 6/30 | 26/30 |

without z-scoring: 6/30. the scale bias is so strong it produces reliable failure in the wrong direction. with z-scoring: 26/30. after standardization, $\text{Var}(\tilde{X}) = \text{Var}(\tilde{Y}) = 1$, so the thresholds are comparable — both networks need to explain the same fraction of variance in their respective targets. the causal signal appears.

why does seed 0 succeed without z-scoring while all other 24 seeds fail? that seed's random initialization happens to generate weight magnitudes that partially compensate for the scale asymmetry. the other seeds don't.

after z-scoring, the minimum losses become:

$$\mathcal{L}^*_\text{fwd} = \frac{\sigma_\varepsilon^2}{\text{Var}(Y)} \quad \text{and} \quad \mathcal{L}^*_\text{rev} = \frac{E[\text{Var}(X \mid Y)]}{\text{Var}(X)}$$

both are now dimensionless fractions of unit variance. the comparison is fair. the causal asymmetry between these two quantities — driven by the noise independence in the forward direction and the entanglement in the reverse direction — becomes the dominant signal.


## the CCA algorithm: what you actually run

two real-valued variables $X$ and $Y$. both z-scored. two MLPs initialized identically (same architecture, same optimizer, same hyperparameters, same random seed). one network $g_\theta$ predicts $Y$ from $X$. one network $h_\phi$ predicts $X$ from $Y$. both train via mini-batch gradient descent with MSE loss, step size $\eta$, batch size $m$, initialized with $\|\theta\|_2, \|\phi\|_2 \leq B$.

training stops when held-out MSE drops below threshold $\tau > 0$, capped at $T_\text{max}$ steps. write $T_\text{fwd}$ for forward convergence steps, $T_\text{rev}$ for reverse. if a network doesn't converge, $T = T_\text{max}$.

the CCA score for edge $X \to Y$:

$$\text{CCA}(X \to Y) = T_\text{fwd} - T_\text{rev}$$

negative score means forward was faster, so predict $X \to Y$. positive score means reverse was faster, so predict $Y \to X$. the magnitude is your confidence: $-2839$ is a strong prediction, $-12$ is a weak one.

for a full causal graph with multiple variables, you run bivariate CCA on each candidate edge and aggregate:

$$\text{CCA}(G) = \sum_{(i,j) \in G} \left[ T_\text{fwd}^{(i,j)} - T_\text{rev}^{(i,j)} \right]$$

this aggregate score feeds into the XGES graph search algorithm as a bonus on top of the MDL score for each candidate edge orientation.


## the canonical experiment: what the numbers look like

$Y = X^3 + \varepsilon$ with $X \sim \mathcal{N}(0,1)$, $\varepsilon \sim \mathcal{N}(0, 0.1)$, $n = 500$ samples, both variables z-scored. architecture: MLP with two hidden layers of 64 neurons each, tanh activation. optimizer: Adam with default learning rate 0.001. threshold $\tau = 0.05$. cap $T_\text{max} = 3000$.

seed 0: forward network converges at step 161. reverse network never converges, hits cap at step 3000. CCA score: $161 - 3000 = -2839$. strong prediction of $X \to Y$.

the forward loss curve shows smooth geometric decay: fast initial decrease when far from minimum (gradient is large, PL says this must be so), slowing as the minimum approaches, crossing $\tau$ at step 161, continuing to decrease. the residuals are becoming cleaner noise as the network learns $f(x) = x^3$.

the reverse loss curve: initial descent for roughly the first few hundred steps, then a plateau just above $\tau = 0.05$, flat for the remaining steps until the cap. this is the non-separable gradient covariance in action: the optimizer descends toward the higher minimum, reaches the vicinity of $\mathcal{L}^*_\text{rev}$, and then position-dependent residuals prevent clean convergence to threshold. the saddle points and flat regions in the reverse landscape (which the local PL analysis doesn't capture globally) cause additional stagnation.

<img width="1200" height="670" alt="fig_loss_curves" src="https://github.com/user-attachments/assets/5073bbb2-f6ed-4776-af4a-84924edf6db4" />


six architectures, five DGPs, five seeds each:

| architecture | $Y = \sin(X)$ | $Y = e^{0.5X}$ | $Y = X^3$ (z-scored) | $Y = X^2$ (boundary) | linear |
|---|---|---|---|---|---|
| MLP-64-64-Tanh / Adam | 5/5 | 5/5 | 4/5 | BC | 0/5 |
| MLP-128-128-Tanh / Adam | 5/5 | 5/5 | 4/5 | BC | 0/5 |
| MLP-32-32-32-Tanh / Adam | 5/5 | 5/5 | 3/5 | BC | 0/5 |
| MLP-64-64-ReLU / Adam | 5/5 | 5/5 | 3/5 | BC | 0/5 |
| MLP-64-64-Tanh / SGD | 5/5 | 5/5 | 5/5 | BC | 0/5 |
| MLP-64-64-Tanh / RMSProp | 5/5 | 5/5 | 4/5 | BC | 0/5 |
| **total** | **30/30** | **30/30** | **23/30** (z-scored) | **BC** | **0/30** |

sine and exponential: 30/30 across all architectures. tanh vs ReLU activations, width 32 vs 128, depth 2 vs 3, Adam vs SGD vs RMSProp — none of it matters. the landscape asymmetry is a property of the objective, not the algorithm.

cubic (z-scored): 23/30. some seeds fail because even with z-scoring, $E[X \mid Y]$ for a symmetric cubic input is near zero, so the reverse minimum is low but the landscape is still rough. the variance across seeds reflects initialization-dependent trajectory differences.

without z-scoring on the cubic, total is 6/30. SGD gets 3/5 while Adam gets 1/5. Adam and RMSProp normalize gradients per-parameter using second moment estimates, which partially neutralizes the raw gradient scale differences that z-scoring is supposed to equalize. SGD's raw gradient magnitudes partially expose the scale asymmetry even without explicit normalization.

linear Gaussian: 0/30. Hoyer et al. (2009, NeurIPS) proved this is unavoidable: the bivariate ANM is identifiable if and only if $f$ is nonlinear or the noise is non-Gaussian. for linear Gaussian, the forward and reverse problems are mirror images. CCA correctly produces scores near zero.

$Y = X^2$: 0/10, wrong systematically for the degenerate collapse reason:

| seed | $T_\text{fwd}$ | $T_\text{rev}$ | result |
|---|---|---|---|
| 0 | 1000 | 9 | wrong |
| 1 | 1000 | 8 | wrong |
| 2 | 1000 | 20 | wrong |
| 3 | 1000 | 21 | wrong |
| 4 | 1000 | 9 | wrong |
| 5 | 1000 | 22 | wrong |
| 6 | 1000 | 25 | wrong |
| 7 | 1000 | 8 | wrong |
| 8 | 1000 | 23 | wrong |
| 9 | 1000 | 8 | wrong |

every single seed: $T_\text{fwd} = 1000$ (hitting cap), $T_\text{rev} \in [8, 25]$. reverse takes fewer than 25 steps because the optimal predictor is identically zero. the injective edge $X_1 \to X_3$ in the same three-variable SCM is correctly identified 10/10. the failure is precisely at the non-injective mechanism and nowhere else.

## the Tübingen benchmark: 96% on real-world data

the Tübingen cause-effect pairs dataset was created by the Max-Planck-Institute for Biological Cybernetics in Tübingen. it contains 108 real-world variable pairs with known ground-truth causal directions, gathered from 37 different domains: altitude and temperature in German weather stations, age and height, ozone and UV radiation, brain weight and body weight, age and blood pressure, barometric pressure and precipitation, horsepower and fuel consumption, and many others. the ground truth is established from domain knowledge and known physical/biological mechanisms. it's been the standard benchmark for bivariate causal discovery methods since Mooij et al. published the definitive study in JMLR 2016. every method in the field gets tested here.

CCA setup: both variables z-scored, MLP-64-64-Tanh/Adam, $\tau = 0.05$, $T_\text{max} = 10{,}000$ steps (longer than synthetic because real mechanisms are messier), 5 random seeds per pair, mean CCA score as direction signal.

| method | accuracy |
|---|---|
| CCA (this paper) | **96%** |
| majority-class baseline | 72.2% |
| ANM / RESIT (Mooij et al. 2016) | 63% |
| IGCI (Janzing & Schölkopf 2010) | ~60% |
| chance | 50% |

96% accuracy. RESIT at 63% is 9 points below the trivial baseline. CCA at 96% is 24 points above the baseline and 33 points above RESIT.

the failure cases cluster at CCA scores near zero. high-confidence predictions (large absolute CCA score) are almost uniformly correct. wrong predictions come from pairs where the mechanism is near-linear or where the marginal distributions are near-symmetric — exactly the boundary conditions the theory predicts. the method's confidence is calibrated: it's uncertain where it should be uncertain.

the longer $T_\text{max} = 10{,}000$ cap matters. synthetic DGPs have controlled noise levels and functional forms. real-world mechanisms can be messier, noisier, or have more complex functional forms. some pairs need more training steps before the asymmetry is visible.

interestingly, ChatGPT achieves ~92.5% on 74 of these pairs when given the variable names and told to reason about causation (from a 2023 study by Sharma et al.). ChatGPT is using world knowledge — it knows altitude causes temperature at weather stations because that's in its training data. CCA achieves 96% using only the numerical data, no variable names, no domain knowledge. it's extracting the causal signal from the statistical pattern alone.

<img width="1712" height="556" alt="fig_tuebingen_final" src="https://github.com/user-attachments/assets/a0cfc433-9f58-43ca-8cf2-ccbf4345c5a3" />


## going beyond bivariate: the CCL framework

CCA solves the bivariate direction problem. the broader paper embeds it in a four-component framework called CCL (Causal Compression Learning) that targets the full pipeline of causal RL: learning the causal graph, building a causally faithful compressed representation, and optimizing a policy that reasons about interventions. understanding CCL requires understanding why each of the four components fails alone.

**MDL alone compresses spurious correlations.** Minimum Description Length follows from Kolmogorov complexity. Kolmogorov complexity $K(x)$ is the length of the shortest program on a universal Turing machine that outputs $x$. Solomonoff proved in 1964 that the optimal prior over hypotheses weights each by $2^{-K(h)}$ — shorter descriptions get exponentially more prior probability. Rissanen (1978) gave the computable approximation: prefer the model minimizing $|M| + |D \mid M|$ (description length of model plus description length of data given model). the Algorithmic Markov Condition (Janzing and Schölkopf 2010) says the true causal factorization $P(X) \cdot P(Y \mid X)$ has shorter Kolmogorov complexity than $P(Y) \cdot P(X \mid Y)$, because independent components compress independently.

the failure mode: when a hidden confounder $Z$ causes both $X$ and $Y$ (no direct $X \to Y$ edge), the shortest description of $P(X, Y)$ captures the $X$-$Y$ correlation. MDL sees the correlation and compresses it, inferring a direct edge, even though the correlation comes entirely through $Z$. MDL needs a causal graph to distinguish confounders from direct effects.

**PAC learning alone certifies statistical, not causal, error.** Valiant's 1984 PAC (Probably Approximately Correct) framework says algorithm $A$ PAC-learns hypothesis class $\mathcal{H}$ if with probability $\geq 1-\delta$ over $n$ samples, it outputs $\hat{h}$ with error $\leq \varepsilon$ when:

$$n \geq C \cdot \frac{d_\text{VC}(\mathcal{H}) + \log(1/\delta)}{\varepsilon^2}$$

VC dimension $d_\text{VC}$ counts the largest set of points the class can shatter (label all $2^m$ ways). it measures statistical capacity. a model that predicts $Y$ from $X$ through a hidden confounder $Z$ (where $Z$ causes both) has the same VC dimension as one using the true direct mechanism, because VC dimension only counts achievable labelings, not whether those labelings reflect genuine causal mechanisms. when you intervene — do$(X = x)$ — the confounder's association with $X$ is severed, and a model that learned through $Z$ fails completely, but the PAC bound is satisfied. PAC bounds on statistical error say nothing about causal error.

CCL replaces $d_\text{VC}$ with $d_c(G)$: the number of edges in the minimal I-map of the causal graph. a minimal I-map is the smallest subgraph that preserves all the conditional independences entailed by the true graph. $d_c(G)$ counts actual causal mechanisms, not statistical capacity. a 20-variable graph with 5 true causal edges has $d_c(G) = 5$. the causal PAC bound becomes:

$$n \geq C \cdot \tau_\text{mix} \cdot d_c(G) \cdot \log(d_c(G)/\delta) \cdot (1-\gamma)^{-3} \cdot \varepsilon^{-2}$$

$\tau_\text{mix}$ is the Markov chain mixing time of the policy trajectory. $(1-\gamma)$ is the discount factor. $d_c(G)$ replaces VC dimension. sparser causal graphs require less data. the $(1-\gamma)^{-3}$ factor compounds three RL-specific penalties from the causal risk decomposition, the concentration inequality, and the RL sample complexity result.

**Pearl's causal framework alone has no unique graph.** the do-calculus is complete — Shpitser and Pearl proved in 2006 that if the calculus can't identify a causal effect, no non-parametric method can. but completeness of the calculus doesn't select the true graph. without a complexity penalty, any fully connected DAG satisfies the identifiability constraints, because you can always add more edges and the do-calculus will still compute interventional distributions (just using more adjustment variables). the framework needs MDL to select among Markov-equivalent graphs.

**causal information bottleneck alone is undefined without a graph.** the standard IB (Tishby, Pereira, and Bialek 2000) compresses input $X$ into representation $T$ to be maximally informative about output $Y$ while being minimally complex:

$$\min_T \; I(X; T) - \beta \cdot I(T; Y)$$

$I(X; T)$ is mutual information between input and representation — you want this small, meaning the representation discards irrelevant details. $I(T; Y)$ is mutual information between representation and output — you want this large, meaning the representation preserves predictive information. $\beta$ trades them off. when $\beta$ is small, compression dominates and the representation is compact. when $\beta$ is large, prediction dominates and the representation is detailed.

the causal blind spot: when a hidden confounder $Z$ causes both $X$ and $Y$, the standard IB preserves $Z$-correlated features in $T$ because they're statistically predictive of $Y$. a clinical trial with healthy patients self-selecting into treatment has "patient health status" correlated with outcome. the IB will compress health status into $T$ because it predicts $Y$ (recovery). the drug might do nothing.

Simoes, Dastani, and van Ommen (2024) fix this by replacing $I(T; Y)$ with causal mutual information:

$$I_c(Y \mid \mathrm{do}(T))$$

the do-operator severs every arrow into $T$. any association that existed only through a shared upstream cause disappears. confounders contribute zero to $I_c$. only the direct causal path $T \to Y$ contributes. the CCL causal IB objective:

$$\min_T \; I(X; T) - \beta \cdot I_c(Y \mid \mathrm{do}(T))$$

to compute $I_c(Y \mid \mathrm{do}(T))$, you need to apply the do-operator, which requires a causal graph. without a graph you can't evaluate $I_c$. the standard IB is computable from data but compresses the wrong thing. the causal IB compresses the right thing but needs a graph first.

CCL closes the loop: MDL needs the graph (from XGES using CCA scores) to flag confounders. Pearl's framework needs MDL to select among Markov-equivalent graphs. causal IB needs the graph to compute $I_c$. PAC bounds use $d_c(G)$ from the graph instead of VC dimension. each component fills what the others miss.


## the CCL+ objective and why every term is there

the full objective:

$$\min_{G, T, \pi} \; \mathcal{L}_\text{CCL+} = \underbrace{-\mathbb{E}_\pi[R(Y)]}_{\text{term 1}} + \underbrace{\lambda_1\bigl[I(X;T) - \beta \cdot I_c(Y \mid \mathrm{do}(T))\bigr]}_{\text{term 2}} + \underbrace{\lambda_2 \cdot \text{MDL}(G)}_{\text{term 3}} + \underbrace{\lambda_3 \cdot \text{CCA}(G)}_{\text{term 4}}$$

subject to: $P(Y \mid \mathrm{do}(\pi))$ identifiable in $G$ via do-calculus.

term 1 is the standard causal RL objective — maximize expected reward under policy $\pi$. the identifiability constraint is hard: if the do-calculus can't compute $P(Y \mid \mathrm{do}(\pi))$ from the current graph structure, the policy optimization step skips for that iteration.

term 2 is the causal IB. minimize $I(X; T)$ (compress, discard irrelevant details of $X$) while maximizing $I_c(Y \mid \mathrm{do}(T))$ (preserve causal signal). $\beta$ trades them off. features predictive through confounders are penalized. features predictive through actual causal mechanisms are rewarded. implemented using a variational autoencoder (VAE) architecture with diagonal Gaussian posterior — the diagonal covariance is mathematically necessary to prevent the encoder from collapsing distinct causal parents into one entangled latent dimension, which would break faithfulness preservation.

term 3 is MDL on graph $G$. approximated via BIC: $\text{MDL}(G) \approx -\ell(G) + \frac{|E_G|}{2}\log n$. log-likelihood of data under graph minus a penalty for the number of edges. drives toward sparser graphs. Kaltenpoth and Vreeken showed that for linear Gaussian SCMs, refined MDL is asymptotically equivalent to BIC, connecting this to classical model selection theory.

term 4 is CCA. enters through the XGES scoring function: $\text{Score}(G) = \text{MDL}(G) + \lambda_3 \cdot \text{CCA}(G)$. CCA adds a bonus (negative score = forward faster = support for $X \to Y$) or penalty (positive score = reverse faster = evidence against $X \to Y$) on top of the MDL score for each candidate edge orientation. XGES then searches for the graph maximizing this combined score.

**theorem 3: the MDL regularization threshold for spurious edge exclusion.** at any local minimum of $\mathcal{L}_\text{CCL}$, if $\lambda_2 \geq \frac{(1-\gamma)\log|V|}{|E_\text{max}|}$, spurious edges are excluded asymptotically as $n \to \infty$. the proof: a spurious edge at finite $n$ can gain at most $O(\log n / n) \to 0$ in MDL likelihood (from the BIC approximation). adding a spurious edge costs at least $(1-\gamma)\log|V|$ in MDL complexity. above the threshold, cost exceeds benefit asymptotically. for the three-variable experiment with $|V| = 3$, $|E_\text{max}| = 3$, $\gamma = 0.9$: threshold $\approx \frac{0.1 \cdot \ln 3}{3} \approx 0.037$. at $\lambda_2 = 0.5$ (well above threshold): zero spurious edges. at $\lambda_2 = 0.1$ (still above threshold): one spurious edge persists because the non-injective $X_2 = X_1^2$ mechanism creates a misleading CCA signal.

**the CCA feedback bound.** for any two graphs differing by a single edge flip:

$$|\mathcal{L}_\text{CCL+}(G') - \mathcal{L}_\text{CCL+}(G)| \leq \frac{\lambda_3 T_\text{max}}{1-\gamma} + \lambda_1 C_\text{IB} + \lambda_2 \log|V|$$

$T_\text{max} < \infty$ by the training cap. $C_\text{IB}$ is a constant from the Lipschitz continuity of $I_c$. this bounds how much the objective can change from one edge flip, which is what descent arguments need. the convergence theorem follows: $\mathcal{L}_\text{CCL+}$ converges to a stationary point under alternating coordinate descent for all $\lambda_3 \geq 0$, proved via: (1) bounded below, (2) XGES accepts only score-decreasing flips, (3) terms 1 and 2 decrease by construction, (4) Zangwill's 1969 theorem applies.


## faithfulness preservation: why the CIB encoder doesn't break graph learning

faithfulness is the assumption that every conditional independence in the data distribution is entailed by a d-separation in the true causal graph. without faithfulness, conditional independence tests used in graph learning (like PC-stable) can't reliably recover the graph.

the concern: the CIB compression step might destroy faithfulness. if the encoder merges two distinct causal parents $X_i$ and $X_j$ into a single latent dimension $T$, the d-separations of the compressed representation might not match the true graph.

theorem F (faithfulness preservation): under four conditions — (1) each identifiable edge has positive causal mutual information, (2) encoder uses diagonal Gaussian covariance, (3) $P(X, Y)$ has full support, (4) encoder converges at rate $O(1/k)$ — as $k \to \infty$, every local minimum of the CIB objective near the optimal encoder satisfies faithfulness between $P(T^{(k)})$ and the true causal graph up to $\varepsilon_G$ in total variation.

proof sketch: forward direction — if $X_i \perp X_j \mid Z$ in the true graph (d-separation), the data processing inequality plus the diagonal covariance assumption (which prevents merging distinct parents) gives $T_i \perp T_j \mid \text{enc}(Z)$ in $P(T)$. reverse direction — if $T_i \perp T_j \mid T_Z$ in $P(T^{(k)})$ but $X_i$ is not d-separated from $X_j$ given $Z$ in the true graph, then by faithfulness of $P(V)$, $I(X_i; X_j \mid Z) > 0$. by convergence of the encoder and continuity of $I_c$, $I(T_i; T_j \mid T_Z) > 0$ for large $k$. contradiction.

the diagonal covariance is doing real work. standard VAEs use diagonal Gaussian posteriors for exactly this reason — factored latent spaces prevent the encoder from collapsing multiple causal parents into one entangled dimension. the local convergence caveat is significant: this holds near the optimal encoder, not globally from arbitrary initialization. other local minima of the CIB objective may exist that don't preserve faithfulness. characterizing global faithfulness is an open problem.

the MDL-R lemma shows that graph learning stays consistent as the encoder evolves: if the encoder converges at rate $\|\text{enc}_k - \text{enc}^*\|_\infty \leq C/k$, the MDL-selected graph converges to the true graph as $n, k \to \infty$ jointly. the proof bounds the perturbation of MDL scores from using $T^{(k)}$ instead of $T^*$ at $O(n/k)$, while the MDL gap between correct and incorrect graphs grows as $O(\log n)$. for $k \geq \sqrt{n}$, the perturbation shrinks faster than the gap, so correct graph recovery probability goes to 1. this is what makes the alternating CCL loop coherent: the representation is always improving, and the graph learning tracks it.

## The four-stage algorithm

**stage 0:** run PC-stable (Colombo and Maathuis 2014) on observational data to recover the undirected skeleton — which pairs are connected, no direction information. PC-stable recovers the skeleton with probability $1-\delta$ for $n \geq O(\log(|V|^2/\delta)/\alpha^2)$ where $\alpha$ is the minimum partial correlation magnitude.

**stage 1:** train the causal IB encoder $T_0$ via variational CIB conditioned on $G_0^\text{skel}$. uses VAE architecture with diagonal covariance. GEM (Generalized EM) property ensures monotone decrease of the CIB objective.

**stage 2:** run XGES with scoring function $\text{Score}(G) = \text{MDL}(G) + \lambda_3 \cdot \text{CCA}(G)$ over $T_0$ and interventional data. XGES (Nazaret and Blei 2024) is an extremely greedy equivalence search extending GES (Chickering 2002) to be computationally more efficient while maintaining correctness guarantees. the CCA scoring for each candidate edge requires training two MLPs and measuring convergence time.

**stage 3:** optimize $\pi_1$ via Bareinboim CRL (2024) over $G_1$ and $T_0$. identifiability check before each policy gradient step.

**alternating loop:** update $T$ (minimize $I(X;T) - \beta I_c(Y \mid \mathrm{do}(T))$), update $G$ (XGES with MDL + CCA), update $\pi$ (policy gradient subject to identifiability), repeat until $|\Delta\mathcal{L}_\text{CCL+}| \leq \varepsilon_\text{conv}$.

output: causal graph $G$, compressed representation $T$, causal policy $\pi$, sample complexity certificate $n^* = C \cdot \hat{\tau}_\text{mix} \cdot d_c(G) \cdot \log(d_c(G)/\delta) / ((1-\gamma)^3\varepsilon^2)$.

<img width="640" height="640" alt="image" src="https://github.com/user-attachments/assets/361e3c17-df54-4bde-b2d9-766068de4d61" />


## Honest accounting

**dimensional scope.** validated entirely on scalar bivariate variables. extending to vector-valued $\mathbf{Y} = f(\mathbf{X}) + \boldsymbol{\varepsilon}$ with $\mathbf{X} \in \mathbb{R}^p$ requires re-examining: injectivity conditions for high-dimensional functions (much harder to verify), the law of total variance (becomes a matrix decomposition), the PL condition (needs multivariate regression network versions), and z-scoring (PCA or whitening might be needed when input dimensions are correlated).

**injectivity requirement.** 0/10 correct on $Y = X^2$. saturating functions in biology, sigmoid-like enzyme kinetics, concave utility functions, threshold effects — many real mechanisms are plausibly non-injective over parts of their domain. a practical CCA deployment needs an injectivity screening test run before committing to a direction. no such test is proposed in the paper.

**local PL condition.** the theorem gives a lower bound using PL locally. the actual empirical gap is 19-fold. the theoretical bound says only that a gap of order $\Omega(1/\eta\mu)$ exists. for quantitative predictions rather than just existence, you'd need global loss landscape characterization, which for neural networks is genuinely hard. recent work on locally PL regions (Aich et al. 2025) gives partial progress but not a full global theory.

**mixing time estimation.** $\tau_\text{mix}$ must be estimated from the observed policy trajectory as a plug-in. standard mixing time estimators from spectral gap methods or empirical trajectory autocorrelation give noisy estimates, and the sample complexity bound $n^*$ is sensitive to $\tau_\text{mix}$.

**interventional data required.** stages 2 and 3 of CCL need interventional data. bivariate CCA works on observational data. the full framework requires actual experiments.

**rung 2 only.** CCL supports interventional reasoning. counterfactual reasoning (rung 3) needs the abduction-action-prediction cycle: infer the background noise variables that produced the observed outcome (abduction), apply the intervention in the modified model (action), compute what would have been produced (prediction). twin-network models implement this. the CCA direction signal is a prerequisite — you need the correct graph before counterfactual inference. extending CCL to rung 3 via twin-network abduction is the natural next step.

**asymptotic guarantees only.** theorems 2, 3, and F hold in the $n \to \infty$ limit. finite-sample convergence rates are not characterized; the finite-sample rate for theorem F is an explicit open problem.


## The thermodynamic connection

in statistical mechanics, entropy-increasing processes are easy to simulate forward. given a current macrostate and the dynamical laws, computing future macrostates is tractable. reconstructing past macrostates from the current one requires inverting dynamics that may have destroyed information. the second law's directionality is fundamental: some information genuinely cannot be recovered.

the noise $\varepsilon$ in $Y = f(X) + \varepsilon$ is added irreversibly. once $Y = f(X) + \varepsilon$ is formed, the information about which specific combination of $X$ and $\varepsilon$ produced this $Y$ is entangled. for injective $f$, the information is present in principle (you could recover $X$ if you knew $\varepsilon$ exactly), but the entanglement makes extraction computationally expensive. the neural network's difficulty learning the reverse direction mirrors the physical difficulty of reconstructing past states from current ones.

the Kolmogorov complexity argument sharpens this. the causal description (independent $P(X)$, mechanism $f$, independent $P(\varepsilon)$) is a shorter program than the anticausal description, because independent things compress independently and entangled things don't. shorter programs are simpler computations. simpler computations converge faster. the forward direction is computationally simpler in a sense that connects to information-theoretic irreversibility, and that simplicity shows up as faster training.

no formal proof connecting neural network optimization landscapes to thermodynamic irreversibility currently exists. but the analogy suggests the convergence asymmetry may extend beyond the specific ANM setting studied here, appearing whenever a generative process has the form "mechanism applied to cause, then independent noise added."


## comparison table

| system | rung 2 capable | PAC bound | causal direction ID | scope |
|---|---|---|---|---|
| GPT-4 / DeepSeek R1 | no | no | no | observational corpus |
| Bareinboim CRL | yes | no | no (takes graph as input) | interventional |
| ANM / RESIT | partial | no | yes | bivariate |
| SkewScore (Lin 2025) | no | no | yes (heteroscedastic) | bivariate |
| CCA / CCL+ | yes (theoretical) | yes (theoretical) | yes | injective ANM |

GPT-4 being in this table isn't a comparison of language models to causal discovery systems. it's to make Pearl's impossibility concrete. GPT-4 is trained on observational data and, by theorem, cannot answer interventional questions from training alone. it can produce fluent text about causation. Bareinboim CRL is the most relevant existing system: it handles rung 2 and out-of-distribution robustness correctly, but takes the causal graph as given input. CCL+ adds the graph learning using CCA for edge orientation, MDL for graph complexity, and causal IB for faithful representations.

the 96% Tübingen result is not just a number. the method fails exactly where the theory says it should fail (near-linear mechanisms, symmetric distributions), succeeds where it should succeed, and is robust to architecture and optimizer changes. methods that work by accident aren't usually robust to both of those. methods that work for the right reason are.


## Experiment 2: does the CCL loop converge?

the convergence theorem says the full CCL+ alternating loop reaches a stationary point. experiment 2 tests this on a concrete three-variable SCM: $X_1 \to X_2 \to X_3$ and $X_1 \to X_3$, with mechanisms $X_2 = X_1^2 + \varepsilon_1$ and $X_3 = X_2 + 0.5X_1 + \varepsilon_2$, $N = 1000$ samples. the test sweeps seven values of $\lambda_2$ (the MDL weight) from 0.01 to 0.5.

| $\lambda_2$ | monotone decrease | $\mathcal{L}$ initial | $\mathcal{L}$ final | iterations | spurious edges |
|---|---|---|---|---|---|
| 0.010 | yes | 10.000 | $-1.238$ | 3 | 1 |
| 0.050 | yes | 10.000 | $-1.106$ | 3 | 1 |
| 0.080 | yes | 10.000 | $-1.007$ | 3 | 1 |
| 0.100 | yes | 10.000 | $-0.942$ | 3 | 1 |
| 0.150 | yes | 10.000 | $-0.777$ | 3 | 1 |
| 0.200 | yes | 10.000 | $-0.612$ | 3 | 1 |
| 0.500 | yes | 10.000 | $+0.105$ | 1 | 0 |

all seven runs show strictly monotone decrease. theorem CCL+ confirmed: the objective goes down every iteration without exception.

the spurious edge story is equally clean. one spurious edge persists at $\lambda_2 \leq 0.2$, and it disappears at $\lambda_2 = 0.5$. why exactly 0.5? the theoretical threshold for this SCM is $\lambda_2 \geq \frac{(1-\gamma)\log|V|}{|E_\text{max}|} \approx 0.037$. 0.5 is well above that threshold. the reason spurious edges survive at lower values in practice despite being above the asymptotic threshold is the non-injective mechanism $X_2 = X_1^2$. recall from the boundary conditions section: the reverse network on that edge collapses to predicting zero in under 25 steps, producing a misleading CCA score that partially offsets the MDL penalty. the spurious edge survives because two errors are compounding. the monotone convergence holds regardless. the theorem says the objective decreases, not that the graph is correct.


## Experiment 4: what the gradient variance actually shows

lemma 2 claims the reverse optimization landscape is structurally harder: higher population minimum, heteroscedastic noise floor, non-separable gradient covariance. experiment 4 measures gradient norm variance directly to test this. the ratio measured is $\sigma^2_{\nabla, \text{rev}} / \sigma^2_{\nabla, \text{fwd}}$, the reverse-to-forward gradient variance ratio, at initialization and after 200 training steps.

| DGP | ratio at initialization | ratio at mid-training |
|---|---|---|
| $Y = X^3 + \varepsilon$ (z-scored) | 0.376 | 0.054 |
| $Y = X^3 + \varepsilon$ (not z-scored) | 0.003 | 0.000 |
| $Y = \sin(X) + \varepsilon$ | 1.161 | 2.179 |
| $Y = e^{0.5X} + \varepsilon$ | 0.684 | 0.276 |
| $Y = X^2 + \varepsilon$ | 0.730 | 0.608 |

the cubic DGP shows ratio below 1 at both phases, even with z-scoring. this initially looks like it contradicts lemma 2. it does not. lemma 2 never claims instantaneous gradient norm variance is larger in the reverse direction at any particular training step. the claim is about landscape structure: higher minimum loss and a non-separable noise floor. these are properties of where the optimizer ends up and how it gets there, not of what the gradient happens to look like at step 200.

the reason the ratio is below 1 for the cubic is a competing effect. the forward network is learning $x \mapsto x^3$, a steeply nonlinear function. the Jacobian norms $\|\nabla_\theta g_\theta\|$ are large because the target function itself has large derivative. the reverse network is trying to learn $E[X \mid Y] \approx 0$, which is nearly constant by symmetry of $P(X)$ for the cubic. near-constant target means small Jacobians, small gradient variance, even though the landscape is harder in the sense of having more structure and a worse noise floor.

the sine DGP is the exception that clarifies everything. its ratio is 1.161 at initialization and grows to 2.179 at mid-training. for the sine, the reverse target $E[X \mid Y]$ is genuinely non-trivial: the sine function is not injective, so multiple branches of the inverse average out into a non-constant function of $Y$. the reverse network has large Jacobians because the target is complex, and the non-separable covariance dominates the Jacobian effect. so the ratio exceeds 1 and the gradient variance story directly confirms lemma 2 for this DGP.

the takeaway: convergence time is the right experimental proxy for landscape asymmetry. gradient variance at a single snapshot is noisy and confounded by the Jacobian structure of the specific function being learned. the 19-fold convergence time gap on the cubic (forward at step 161, reverse capped at 3000) is the signal the theory is about, and no single snapshot of gradient statistics captures it.


## what this could actually do in the real world

CCA and CCL are validated on synthetic and benchmark data. the path to real-world use is direct. every domain below has the same pattern: a confounded observational dataset, a question about which direction the arrow points, and a practical barrier that is a specific extension of the existing theory rather than a fundamental departure from it.

**medicine and drug development.** the clearest application is distinguishing drug effects from patient selection effects. patients who take a drug are not random: they tend to be sicker, or healthier, or richer, depending on the treatment. the observed correlation between drug and outcome mixes the actual causal effect with the selection pattern. CCA could identify whether a biomarker causes an outcome or the outcome drives the biomarker, without requiring a randomized trial. what needs to change: real biological mechanisms are often saturating or threshold-like, meaning approximately non-injective in parts of their range. an injectivity screening step before CCA direction scoring is the primary practical requirement.

**economics and policy evaluation.** does education cause higher earnings, or do families with resources invest in both simultaneously? does a minimum wage increase cause unemployment, or do strong labor markets adopt higher wages first? instrumental variable methods exist for these questions but require strong assumptions about the instrument. CCA offers a model-free approach that makes different assumptions. the requirement is an approximately nonlinear injective mechanism, which is plausible for many economic relationships in the range of observed variation.

**genetics and gene regulation.** does gene A regulate gene B, or is their co-expression from a shared upstream factor? current approaches use Mendelian randomization or direct perturbation experiments. CCA on observational expression data could prioritize which edges to target before running expensive experiments. the CCL confounding framework is specifically designed for the shared-genetic-background problem, where unmeasured common causes produce spurious correlations between genes with no direct relationship.

**climate and environmental science.** does $\text{CO}_2$ drive temperature, or does temperature drive $\text{CO}_2$ release through permafrost thaw and ocean outgassing? both, actually, which is a feedback loop. extending CCL to time series with feedback requires additional theory beyond the static ANM setting here, but the core signal is the same: which direction has the more asymmetric convergence profile.

the common thread is that none of these requires rebuilding the method from scratch. each one is a specific extension: handle approximately non-injective mechanisms, scale to multivariate inputs, extend to time series. the core CCA theorem is a fixed point from which these extensions proceed.


## Toward rung 3

CCL operates on rung 2. rung 3 requires counterfactual reasoning: you observed outcome $Y = y$ under treatment $X = x$. what would $Y$ have been under treatment $X = x'$? this is $P(Y_{X=x'} \mid X = x, Y = y)$. you can never observe both worlds simultaneously.

Pearl's abduction-action-prediction cycle handles this. step one, abduction: given the observed data and the causal model, infer the specific values of the background noise variables $U$ that would have produced the observation. these are the "latent state of the world" at the time of observation. step two, action: modify the model by forcing $X$ to $x'$, leaving the inferred $U$ fixed. step three, prediction: compute the output $Y$ from the modified model. the result is the counterfactual.

twin-network models implement this computationally. you run two copies of the SCM simultaneously: the actual world and the counterfactual world. they share the inferred noise variables $U$ (same background circumstances) but differ in the intervention applied. the counterfactual outcome is read off the second copy.

the CCL framework is a prerequisite for this, not an alternative to it. you need the correct causal graph before you can do counterfactual inference. abduction requires knowing the structural equations well enough to invert them for $U$. CCA gives you the edge directions. CCL gives you the full graph structure and the compressed faithful representation. twin-network counterfactuals sit on top of that infrastructure. the natural extension of CCL to rung 3 is to add a twin-network abduction module as a fourth stage after policy optimization, sharing the causal graph and encoder learned in stages 0 to 3.

the work here is foundational in this sense: it builds the scaffold for rung 2, which is the necessary first step toward rung 3. if you can identify which direction the arrow points and learn the graph structure faithfully, the machinery for counterfactual reasoning can be built on top.


## Conclusion

the central observation is simple. training a network to predict $Y$ from $X$ is easier than training a network to predict $X$ from $Y$, when $X$ is the actual cause. that is not an accident. it is a structural consequence of how the data was generated: the forward direction has independent noise, the reverse does not.

lemma 1 says reverse residuals stay correlated with the input at any finite approximation. lemma 2 says that correlation creates a higher minimum loss and a non-separable gradient noise floor. lemma 3 says a harder landscape requires more steps under PL. chain complete: $E[T_\text{fwd}] < E[T_\text{rev}]$.

the experiments validate the theorem and its limits at the same time. 30/30 on sine and exponential across six architectures. 26/30 on the cubic with z-scoring. the linear Gaussian and non-injective failures happen exactly as predicted before the experiments were run. the Tubingen benchmark gives 96%, 33 points above RESIT and 24 above the majority baseline. the method knows when it is uncertain: wrong predictions cluster near CCA score zero, which is the correct behavior.

the CCL framework embeds CCA in a larger structure with five supporting theorems: MDL efficiency inheritance, linear complexity reduction, the causal PAC bound, faithfulness preservation under CIB compression, and CCL+ convergence. all five are proved. empirical validation on full causal benchmarks beyond Tubingen, the extension to multivariate mechanisms, and the extension to rung 3 counterfactuals are the primary remaining steps.

*research: "Causal Direction from Convergence Time: Faster Training in the True Causal Direction," Abdulrahman Tamim, February 2026.*


---

## References

[1] J. Pearl, "Causal diagrams for empirical research," *Biometrika*, vol. 82, no. 4, pp. 669–688, 1995.

[2] J. Pearl, *Causality: Models, Reasoning, and Inference*. Cambridge University Press, 2000.

[3] Y. Huang and M. Valtorta, "Pearl's calculus of intervention is complete," in *Proc. UAI*, 2006.

[4] R. Solomonoff, "A formal theory of inductive inference," *Information and Control*, vol. 7, no. 1, pp. 1–22, 1964.

[5] A. N. Kolmogorov, "Three approaches to the quantitative definition of information," *Problems of Information and Transmission*, vol. 1, no. 1, pp. 1–7, 1965.

[6] J. Rissanen, "Modeling by shortest data description," *Automatica*, vol. 14, no. 5, pp. 465–471, 1978.

[7] L. G. Valiant, "A theory of the learnable," *Commun. ACM*, vol. 27, no. 11, pp. 1134–1142, 1984.

[8] S. Hanneke, "The optimal sample complexity of PAC learning," *JMLR*, vol. 17, no. 38, pp. 1–15, 2016.

[9] N. Tishby, F. C. Pereira, and W. Bialek, "The information bottleneck method," *arXiv:physics/0004057*, 2000.

[10] F. N. F. Q. Simoes, M. Dastani, and T. van Ommen, "The causal information bottleneck and optimal causal variable abstractions," *arXiv:2410.00535*, 2024. (To appear in *Proc. UAI 2025*.)

[11] E. Bareinboim, J. Zhang, and S. Lee, "An introduction to causal reinforcement learning," *Foundations and Trends in Machine Learning*, vol. 17, no. 3, pp. 304–589, 2024.

[12] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, "Deep variational information bottleneck," in *Proc. ICLR*, 2017.

[13] D. Kaltenpoth and J. Vreeken, "Causal discovery with hidden confounders using the algorithmic Markov condition," in *Proc. UAI*, 2023.

[14] P. Grünwald, *The Minimum Description Length Principle*. MIT Press, 2007.

[15] G. Schwarz, "Estimating the dimension of a model," *Ann. Stat.*, vol. 6, no. 2, pp. 461–464, 1978.

[16] P. Bühlmann, J. Peters, and J. Ernest, "CAM: Causal additive models, high-dimensional order search and penalized regression," *Ann. Stat.*, vol. 42, no. 6, pp. 2526–2556, 2014.

[17] M. Li and P. Vitányi, *An Introduction to Kolmogorov Complexity and Its Applications*, 3rd ed. Springer, 2008.

[18] N. K. Vereshchagin and P. M. B. Vitányi, "Kolmogorov's structure functions and model selection," *IEEE Trans. Inf. Theory*, vol. 50, no. 12, pp. 3265–3290, 2004.

[19] D. Janzing and B. Schölkopf, "Causal inference using the algorithmic Markov condition," *IEEE Trans. Inf. Theory*, vol. 56, no. 10, 2010.

[20] P. Spirtes, C. Glymour, and R. Scheines, *Causation, Prediction, and Search*, 2nd ed. MIT Press, 2000.

[21] D. M. Chickering, "Optimal structure identification with greedy search," *JMLR*, vol. 3, pp. 507–554, 2002.

[22] M. Nazaret and D. Blei, "XGES: Extremely greedy equivalence search for causal discovery," in *Proc. ICML*, 2024.

[23] D. Colombo and M. H. Maathuis, "Order-independent constraint-based causal structure learning," *JMLR*, vol. 15, no. 1, pp. 3741–3782, 2014.

[24] I. Shpitser and J. Pearl, "Identification of joint interventional distributions in recursive semi-Markovian causal models," in *Proc. AAAI*, 2006.

[25] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth, "Learnability and the Vapnik-Chervonenkis dimension," *JACM*, vol. 36, no. 4, pp. 929–965, 1989.

[26] M. G. Azar, R. Munos, and H. J. Kappen, "Minimax PAC bounds on the sample complexity of reinforcement learning," *Machine Learning*, vol. 91, no. 3, pp. 325–349, 2013.

[27] D. A. Levin and Y. Peres, *Markov Chains and Mixing Times*, 2nd ed. American Mathematical Society, 2017.

[28] M. Mohri and V. Kuznetsov, "Generalization bounds for non-stationary mixing processes," *JMLR*, 2018.

[29] W. I. Zangwill, *Nonlinear Programming: A Unified Approach*. Prentice-Hall, 1969.

[30] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, "An introduction to variational methods for graphical models," *Machine Learning*, vol. 37, pp. 183–233, 1999.

[31] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, "Policy gradient methods for reinforcement learning with function approximation," in *Proc. NeurIPS*, 1999.

[32] B. Yu, "Rates of convergence for empirical processes of stationary mixing sequences," *Ann. Probab.*, vol. 22, no. 1, pp. 94–116, 1994.

[33] OpenAI, "GPT-4 technical report," *arXiv:2303.08774*, 2023.

[34] DeepSeek, "DeepSeek-V3 technical report," *arXiv:2412.19437*, 2025.

[35] J. Peters, J. M. Mooij, D. Janzing, and B. Schölkopf, "Causal discovery with continuous additive noise models," *JMLR*, vol. 15, pp. 2009–2053, 2014.

[36] Y. Lin, Y. Huang, W. Liu, H. Deng, I. Ng, K. Zhang, M. Gong, Y.-A. Ma, and B. Huang, "A skewness-based criterion for addressing heteroscedastic noise in causal discovery," in *Proc. ICLR*, 2025. (*arXiv:2410.06407*)

[37] J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Schölkopf, "Distinguishing cause from effect using observational data: Methods and benchmarks," *JMLR*, vol. 17, no. 32, pp. 1–102, 2016.

[38] L. Bottou, "Large-scale machine learning with stochastic gradient descent," in *Proc. COMPSTAT*, pp. 177–186, 2010.

[39] A. P. Dempster, N. M. Laird, and D. B. Rubin, "Maximum likelihood from incomplete data via the EM algorithm," *J. Roy. Statist. Soc. Ser. B*, vol. 39, no. 1, pp. 1–22, 1977.

[40] C. F. J. Wu, "On the convergence properties of the EM algorithm," *Ann. Statist.*, vol. 11, no. 1, pp. 95–103, 1983.

[41] K. Ji, J. Yang, and Y. Liang, "Bilevel optimization: Convergence analysis and enhanced design," in *Proc. ICML*, pp. 4882–4892, PMLR, 2021.
